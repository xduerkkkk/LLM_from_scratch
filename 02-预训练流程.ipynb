{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 说明：\n",
    "本次训练完全自己造轮子\n",
    "不使用transformer的trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立整体框架\n",
    "我们先不写具体代码，先把大框架设计好，再往里面填东西\n",
    "*配置与参数*：\n",
    "- 神经"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配置与参数解析\n",
    "我们首先要设计好，训练的一些参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_minimind\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MiniMindConfig\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTrainArgs\u001b[39;00m():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#文件管理\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     out_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./pretrain_output\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m     checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./pretrain_output/latest_checkpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mTrainArgs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m grad_clip \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 15\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m log_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from model.model_minimind import MiniMindConfig\n",
    "class TrainArgs():\n",
    "    #文件管理\n",
    "    out_dir = './pretrain_output'\n",
    "    checkpoint_path = \"./pretrain_output/latest_checkpoint.pth\"\n",
    "    data_path = '../data/pretrain_hq.jsonl'\n",
    "    #神经网络训练管理\n",
    "    epochs = 2\n",
    "    batch_size = 16\n",
    "    accumulation_steps = 4\n",
    "    learning_rate = 5e-4\n",
    "    warm_up = 0\n",
    "    grad_clip = 1\n",
    "    dtype = 'bfloat16'\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers = 0\n",
    "    log_interval = 50\n",
    "    save_interval = 2000\n",
    "    ctx = nullcontext() if device == \"cpu\" else torch.cuda.amp.autocast()\n",
    "class LLMargs():\n",
    "    use_moe = True\n",
    "    hidden_size = 512\n",
    "    num_hidden_layers = 8\n",
    "    \n",
    "\n",
    "lm_config = MiniMindConfig(use_moe=LLMargs.use_moe,hidden_size=LLMargs.hidden_size,num_hidden_layers=LLMargs.num_hidden_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(TrainArgs.save_dir, exist_ok=True)\n",
    "os.makedirs(TrainArgs.out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境与设备初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "Number of GPUs: 1\n",
      "  GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "    Memory: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型加载、数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lm_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_minimind\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MiniMindConfig, MiniMindForCausalLM\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../model/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m MiniMindForCausalLM(\u001b[43mlm_config\u001b[49m)\u001b[38;5;241m.\u001b[39mto(TrainArgs\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lm_config' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained('../model/')\n",
    "model = MiniMindForCausalLM(lm_config).to(TrainArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniMindForCausalLM(\n",
      "  (model): MiniMindModel(\n",
      "    (embed_tokens): Embedding(6400, 512)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x MiniMindBlock(\n",
      "        (self_attn): Attention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "        (mlp): MOEFeedForward(\n",
      "          (experts): ModuleList(\n",
      "            (0-3): 4 x FeedForward(\n",
      "              (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "              (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (gate): MoEGate()\n",
      "          (shared_experts): ModuleList(\n",
      "            (0): FeedForward(\n",
      "              (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "              (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=6400, bias=False)\n",
      ")\n",
      "模型总参数量 (Total Parameters): 95,052,288\n",
      "可训练参数量 (Trainable Parameters): 95,052,288\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# 打印结果，使用 f-string 的格式化功能让数字更易读（例如，加上千位分隔符）\n",
    "print(f\"模型总参数量 (Total Parameters): {total_params:,}\")\n",
    "\n",
    "# 如果你还想区分“可训练参数”，可以这样做：\n",
    "# 在大多数情况下，总参数量和可训练参数量是一样的。\n",
    "# 除非你手动设置了某些参数的 requires_grad=False (冻结了某些层)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"可训练参数量 (Trainable Parameters): {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于 Dataset 类，它通常要求其子类至少实现两个特殊方法：\n",
    "\n",
    "`__len__(self)`: 这个方法应该返回数据集中样本的总数量。PyTorch 的 DataLoader 会调用它来知道数据集的大小。\n",
    "\n",
    "`__getitem__(self, index)`: 这个方法是核心，它应该根据给定的 index（索引）返回数据集中的一个样本。DataLoader 会通过这个方法来逐个或批量获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义处理数据的类\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import json\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self,tokenizer,data_path,max_length=512):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = self.load_data(data_path)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_data(self,data_path):\n",
    "        samples = []\n",
    "        with open(data_path,'r',encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                samples.append(data)\n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        sample = self.samples[index]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            sample['text'],\n",
    "            max_length = self.max_length,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding.input_ids.squeeze()\n",
    "        loss_mask = (input_ids != self.tokenizer.pad_token_id)\n",
    "\n",
    "        X = input_ids[:-1].clone().detach()\n",
    "        Y = input_ids[1:].clone().detach()\n",
    "        loss_mask = loss_mask[1:].clone().detach()\n",
    "\n",
    "        return X,Y,loss_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PretrainDataset(tokenizer,TrainArgs.data_path,max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([   1, 1227, 2346, 5914,  270, 6095,  801, 1415, 1232, 5354,  286,  201,\n",
      "         434, 1157, 2042, 2504, 1422, 3458,  854,  888, 2537, 5371,  801, 1415,\n",
      "        1232, 5354,   28,  223, 2037,  595,   17,  854,  888, 2537, 3458, 5923,\n",
      "           2,  223,    1, 1086,  397, 5885, 2336, 3694,  269, 3858, 1994,  286,\n",
      "        6243,  270, 5213, 1086,  803, 5885,  682, 2711, 3858, 1994,  286, 4600,\n",
      "         803,  590, 1698, 3445, 2404,  371, 1971,  269, 3858, 1994, 3274,  814,\n",
      "           2,  223,    1, 1086,  397, 2910,  541, 5804,  953,  854, 5221,  242,\n",
      "        1531,  286, 1188,  270,  397, 1086,  803, 2910,  541, 2345, 1439, 1330,\n",
      "         237, 5221,  242, 1531,  286, 4587, 1439, 1330,  237, 3993,  568, 1687,\n",
      "         814, 1775, 4874, 1170,  345, 1476,  953,  446, 2207,    2,  223,    1,\n",
      "        2525, 4584, 2706,  333, 1581, 2345,  611, 1781,  269, 3044, 1486, 3411,\n",
      "         286, 1944,  103, 3344, 3139,  876,  118, 1749, 1729,  954,  270, 1781,\n",
      "         954,  845, 1468, 1319,  954,  537,  708,  286, 3344, 1035,  252, 1209,\n",
      "         251, 1209,  251,  694,  256, 2442, 1039,  270, 1781, 4148, 2133,  112,\n",
      "        2133,  245, 1395, 1749,  457,  113,  286,    2,  223,    1, 4191,  608,\n",
      "         270,  434, 2042, 2239, 1046,  400,  411,  286,  453, 1294,  963,  270,\n",
      "         397, 1046,  654, 2486, 1237, 4892,  991, 1190,  286, 3989, 3227, 1437,\n",
      "         812,  673, 1825,  270, 5854,  451,  510,  397, 1115,  286,    2,  223,\n",
      "           1, 5134,  270, 3589,  572, 6250,  541, 1703, 4029, 1809, 4379, 4025,\n",
      "         270,  608, 5183, 3276, 2235,  814,  803,  587, 2207, 6243,  270, 5712,\n",
      "        1008,  286,  803, 4720, 1683, 1607, 4025, 4217, 3889,  800, 3135, 1607,\n",
      "         814,    2,  223,    1, 4587, 3308, 1938,  453, 2947, 2826, 2103,  814,\n",
      "        6256, 1949, 5889,  400, 2925,  270, 2075, 2346, 1327,  400, 2925,  270,\n",
      "         590, 6323, 1439, 1110, 1410, 1949, 5889,  315, 2346, 1327,  286,    2,\n",
      "         223,    1, 4191, 2155, 2212,  286, 2525, 3589, 3115,  434,  962, 3524,\n",
      "         269, 2412, 6029,  537,  829,  286,  434,  962, 3524, 3699,  345, 6210,\n",
      "        1507, 1655, 1749,   22, 1395,  270,  537,  829,  345,  568, 1781,  286,\n",
      "           2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0]), tensor([1227, 2346, 5914,  270, 6095,  801, 1415, 1232, 5354,  286,  201,  434,\n",
      "        1157, 2042, 2504, 1422, 3458,  854,  888, 2537, 5371,  801, 1415, 1232,\n",
      "        5354,   28,  223, 2037,  595,   17,  854,  888, 2537, 3458, 5923,    2,\n",
      "         223,    1, 1086,  397, 5885, 2336, 3694,  269, 3858, 1994,  286, 6243,\n",
      "         270, 5213, 1086,  803, 5885,  682, 2711, 3858, 1994,  286, 4600,  803,\n",
      "         590, 1698, 3445, 2404,  371, 1971,  269, 3858, 1994, 3274,  814,    2,\n",
      "         223,    1, 1086,  397, 2910,  541, 5804,  953,  854, 5221,  242, 1531,\n",
      "         286, 1188,  270,  397, 1086,  803, 2910,  541, 2345, 1439, 1330,  237,\n",
      "        5221,  242, 1531,  286, 4587, 1439, 1330,  237, 3993,  568, 1687,  814,\n",
      "        1775, 4874, 1170,  345, 1476,  953,  446, 2207,    2,  223,    1, 2525,\n",
      "        4584, 2706,  333, 1581, 2345,  611, 1781,  269, 3044, 1486, 3411,  286,\n",
      "        1944,  103, 3344, 3139,  876,  118, 1749, 1729,  954,  270, 1781,  954,\n",
      "         845, 1468, 1319,  954,  537,  708,  286, 3344, 1035,  252, 1209,  251,\n",
      "        1209,  251,  694,  256, 2442, 1039,  270, 1781, 4148, 2133,  112, 2133,\n",
      "         245, 1395, 1749,  457,  113,  286,    2,  223,    1, 4191,  608,  270,\n",
      "         434, 2042, 2239, 1046,  400,  411,  286,  453, 1294,  963,  270,  397,\n",
      "        1046,  654, 2486, 1237, 4892,  991, 1190,  286, 3989, 3227, 1437,  812,\n",
      "         673, 1825,  270, 5854,  451,  510,  397, 1115,  286,    2,  223,    1,\n",
      "        5134,  270, 3589,  572, 6250,  541, 1703, 4029, 1809, 4379, 4025,  270,\n",
      "         608, 5183, 3276, 2235,  814,  803,  587, 2207, 6243,  270, 5712, 1008,\n",
      "         286,  803, 4720, 1683, 1607, 4025, 4217, 3889,  800, 3135, 1607,  814,\n",
      "           2,  223,    1, 4587, 3308, 1938,  453, 2947, 2826, 2103,  814, 6256,\n",
      "        1949, 5889,  400, 2925,  270, 2075, 2346, 1327,  400, 2925,  270,  590,\n",
      "        6323, 1439, 1110, 1410, 1949, 5889,  315, 2346, 1327,  286,    2,  223,\n",
      "           1, 4191, 2155, 2212,  286, 2525, 3589, 3115,  434,  962, 3524,  269,\n",
      "        2412, 6029,  537,  829,  286,  434,  962, 3524, 3699,  345, 6210, 1507,\n",
      "        1655, 1749,   22, 1395,  270,  537,  829,  345,  568, 1781,  286,    2,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0]), tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False]))\n",
      "长度为1413103\n"
     ]
    }
   ],
   "source": [
    "print(f'{train_ds[1]}\\n长度为{len(train_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想检验整个训练流程，采用小数据检验是最合适的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "长度为500\n"
     ]
    }
   ],
   "source": [
    "test_ds = Subset(train_ds, range(500))\n",
    "print(f'\\n长度为{len(test_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size = 4,\n",
    "    shuffle = True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size = TrainArgs.batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = TrainArgs.num_workers,\n",
    "    pin_memory= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "  X_batch shape: torch.Size([8, 511])\n",
      "  Y_batch shape: torch.Size([8, 511])\n",
      "  loss_mask_batch shape: torch.Size([8, 511])\n"
     ]
    }
   ],
   "source": [
    "tag = False\n",
    "for batch_idx, (X_batch, Y_batch, loss_mask_batch) in enumerate(train_loader):\n",
    "    if not tag:\n",
    "        print(f\"Batch {batch_idx}:\")\n",
    "        print(f\"  X_batch shape: {X_batch.shape}\")\n",
    "        print(f\"  Y_batch shape: {Y_batch.shape}\")\n",
    "        print(f\"  loss_mask_batch shape: {loss_mask_batch.shape}\")\n",
    "        tag = True\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "  X_batch shape: torch.Size([4, 511])\n",
      "  Y_batch shape: torch.Size([4, 511])\n",
      "  loss_mask_batch shape: torch.Size([4, 511])\n"
     ]
    }
   ],
   "source": [
    "tag = False\n",
    "for batch_idx, (X_batch, Y_batch, loss_mask_batch) in enumerate(test_loader):\n",
    "    if not tag:\n",
    "        print(f\"Batch {batch_idx}:\")\n",
    "        print(f\"  X_batch shape: {X_batch.shape}\")\n",
    "        print(f\"  Y_batch shape: {Y_batch.shape}\")\n",
    "        print(f\"  loss_mask_batch shape: {loss_mask_batch.shape}\")\n",
    "        tag = True\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练核心五步法\n",
    "- 数据上设备\n",
    "- 前向传播\n",
    "- 计算损失\n",
    "- 反向传播\n",
    "- 更新权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import optim\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n",
    "optimizer = optim.AdamW(model.parameters(), lr=TrainArgs.learning_rate)\n",
    "#据模型在训练数据上的表现来调整模型的参数  训练的一个大脑\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(TrainArgs.dtype in ['float16', 'bfloat16']))\n",
    "#自动混合精度训练工具 训练的一个助手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Tracking run with swanlab version 0.6.7                                   \n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Run data will be saved locally in \u001b[35m\u001b[1m/home/kkkkkk/ai_project/minimind/从0到1大模型全流程梳理/swanlog/run-20250719_204458-ebf8235a\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 👋 Hi \u001b[1m\u001b[39mkkaiaiai\u001b[0m\u001b[0m, welcome to swanlab!\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: Syncing run \u001b[33mMiniMind-Pretrain\u001b[0m to the cloud\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🏠 View project at \u001b[34m\u001b[4mhttps://swanlab.cn/@kkaiaiai/MiniMind-Pretrain\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[34mswanlab\u001b[0m\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://swanlab.cn/@kkaiaiai/MiniMind-Pretrain/runs/xtpiiakzeio6vgsef1zs6\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>Show Iframe</title>\n",
       "    \n",
       "        <script>\n",
       "            function showIframe() {\n",
       "                var iframeHtml = '<iframe src=\"https://swanlab.cn/@kkaiaiai/MiniMind-Pretrain/runs/xtpiiakzeio6vgsef1zs6\" width=100% height=\"600\" frameborder=\"no\"></iframe>';\n",
       "                document.getElementById('iframeContainer').innerHTML = iframeHtml;\n",
       "            }\n",
       "        </script>\n",
       "        \n",
       "</head>\n",
       "<body>\n",
       "    <style>\n",
       "        .interactive-button {\n",
       "            display: flex;\n",
       "            align-items: center;\n",
       "            height: 36px;\n",
       "            border: 0px;\n",
       "            background-color: #2c8f63;\n",
       "            color: white;\n",
       "            padding: 10px 20px;\n",
       "            transition: background-color 0.3s, transform 0.2s;\n",
       "        }\n",
       "\n",
       "        .interactive-button:hover {\n",
       "            background-color: #5cab87;\n",
       "            cursor: pointer;\n",
       "        }\n",
       "\n",
       "        .interactive-button:active { background-color: #217952; transform: scale(0.96); } </style> <br> <button \n",
       "        onclick=\"showIframe()\" class=\"interactive-button\"> <svg style=\"height: 16px; margin-right: 8px;\" viewBox=\"0 0 \n",
       "        46 46\" fill=\"none\"> <path d=\"M10.8439 21.1974C10.6414 21.2854 10.4477 21.3925 10.2655 21.5173L10.2069 \n",
       "        21.5652C10.1839 21.58 10.1625 21.5969 10.1429 21.6159C6.29135 24.6118 4.22831 29.4416 5.32646 34.282C5.94656 \n",
       "        37.0577 7.50461 39.5348 9.73801 41.2958C11.9714 43.0568 14.7436 43.994 17.5874 43.9495H18.0219C19.8864 \n",
       "        43.8697 21.7087 43.3694 23.3526 42.486C24.9964 41.6026 26.4193 40.3589 27.5147 38.848C28.61 37.3371 29.3496 \n",
       "        35.598 29.678 33.761C30.0065 31.9239 29.9153 30.0363 29.4112 28.2395C28.9181 26.4723 27.8919 24.8437 26.9937 \n",
       "        23.2551C25.4158 20.4653 23.8343 17.6764 22.2492 14.8884C21.7801 14.0647 21.3057 13.2465 20.8419 \n",
       "        12.4228C20.2315 11.3353 19.2746 10.1519 19.224 8.86183C19.1733 7.57176 20.2235 6.32701 21.5082 \n",
       "        6.07912C23.9284 5.61801 25.0639 8.24078 25.0693 8.23812C25.363 8.94035 25.9123 9.50489 26.6063 \n",
       "        9.81764C27.3002 10.1304 28.087 10.168 28.8077 9.92298C29.5283 9.67791 30.1291 9.1684 30.4885 8.49743C30.8479 \n",
       "        7.82646 30.9392 7.04405 30.7439 6.30835C30.1514 4.37314 28.9133 2.69953 27.2363 1.56656C25.7615 0.511704 \n",
       "        23.9847 -0.0372109 22.1719 0.00195984C20.9049 0.00893199 19.6532 0.27989 18.4967 0.797557C17.3402 1.31522 \n",
       "        16.3043 2.06823 15.4551 3.00856C14.49 4.08707 13.7984 5.38193 13.4389 6.78385C13.0794 8.18576 13.0624 9.6536 \n",
       "        13.3894 11.0635C13.52 11.593 13.6984 12.1095 13.9225 12.6067C14.5595 14.0514 15.4951 15.3681 16.284 \n",
       "        16.7355C17.2525 18.4147 18.2209 20.0948 19.1893 21.7758C20.1578 23.4568 21.1351 25.1449 22.1213 \n",
       "        26.8401C22.9209 28.2421 23.7925 29.4682 23.8805 31.1528C23.9175 32.0513 23.7682 32.9479 23.4419 \n",
       "        33.7859C23.1156 34.6239 22.6194 35.3854 21.9845 36.0223C21.3496 36.6592 20.5897 37.1578 19.7527 \n",
       "        37.4868C18.9157 37.8157 18.0196 37.9678 17.121 37.9336C14.0024 37.7923 11.6488 35.4814 11.1744 32.4588C10.58 \n",
       "        28.6419 13.552 26.5469 13.552 26.5469C14.1782 26.1785 14.6497 25.5955 14.8791 24.906C15.1084 24.2166 15.0801 \n",
       "        23.4673 14.7993 22.7971C14.5186 22.127 14.0044 21.5813 13.3521 21.2611C12.6998 20.941 11.9536 20.8682 11.2517 \n",
       "        21.0561C11.1174 21.0939 10.9856 21.1402 10.8572 21.1947\" fill=\"white\" /> <path d=\"M42.8101 31.5968C42.8109 \n",
       "        30.5198 42.7218 29.4445 42.5435 28.3823C42.2663 26.7069 41.7464 25.0808 41.0002 23.5552C40.5524 22.6463 \n",
       "        39.9874 21.7374 39.1024 21.2417C38.6593 20.9919 38.1589 20.8617 37.6502 20.8639C37.1416 20.8661 36.6423 \n",
       "        21.0006 36.2013 21.2541C35.7604 21.5077 35.393 21.8716 35.1352 22.3101C34.8775 22.7485 34.7382 23.2466 \n",
       "        34.7312 23.7552C34.7072 24.8773 35.3149 25.8875 35.768 26.9217C36.5212 28.6453 36.8623 30.5208 36.7642 \n",
       "        32.3993C36.6661 34.2777 36.1315 36.1075 35.2029 37.7433C35.146 37.8404 35.0952 37.941 35.051 38.0445C34.8623 \n",
       "        38.4842 34.7635 38.9573 34.7605 39.4358C34.7802 40.1222 35.0356 40.7808 35.4835 41.3011C35.9315 41.8214 \n",
       "        36.5449 42.1717 37.2207 42.2932C38.8759 42.589 40.1899 41.347 40.8856 39.9609C42.1643 37.3589 42.823 34.4961 \n",
       "        42.8101 31.5968Z\" fill=\"white\" /> <path d=\"M28.2309 11.8938C28.1761 11.9043 28.1218 11.9176 28.0683 \n",
       "        11.9338C27.9593 11.9642 27.8611 12.0249 27.7851 12.1088C27.7091 12.1928 27.6584 12.2965 27.6389 \n",
       "        12.408C27.6193 12.5195 27.6318 12.6343 27.6748 12.7391C27.7178 12.8438 27.7895 12.9343 27.8818 \n",
       "        12.9999C29.2375 14.0252 30.3809 15.3043 31.2482 16.7662C31.4838 17.1677 31.6888 17.5865 31.8612 \n",
       "        18.0189C32.0052 18.3921 32.1971 18.8799 32.6822 18.8532C33.0607 18.8346 33.2153 18.512 33.3192 \n",
       "        18.1895C33.8137 16.5125 33.9678 14.7534 33.7723 13.0159C33.6331 12.0693 33.4155 11.1359 33.122 \n",
       "        10.2252C33.0775 10.0047 32.9744 9.80029 32.8235 9.6335C32.7273 9.54627 32.6054 9.49262 32.4761 9.4806C32.3468 \n",
       "        9.46859 32.2171 9.49886 32.1065 9.56687C32.0016 9.65188 31.9115 9.75365 31.8399 9.86806C31.3956 10.4658 \n",
       "        30.825 10.9581 30.1687 11.3101C29.8377 11.4861 29.4893 11.6272 29.1292 11.7312C28.828 11.8192 28.5215 11.8325 \n",
       "        28.2309 11.8938Z\" fill=\"white\" /> </svg> Display SwanLab Board </button> <br> <div \n",
       "        id=\"iframeContainer\"></div> </body> </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<swanlab.data.run.main.SwanLabRun at 0x7fa74f50e710>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import swanlab\n",
    "# 初始化swanlab，传入项目名、实验名等\n",
    "swanlab.init(\n",
    "    project=\"MiniMind-Pretrain\", \n",
    "    experiment_name=\"MiniMind-Pretrain\", \n",
    "    config=vars(TrainArgs()) # 将你的所有超参数配置一次性传给swanlab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(epoch, model, train_loader, optimizer, scaler, lm_config):\n",
    "    #选择初始化损失函数！\n",
    "    model.train() # 确保模型处于训练模式\n",
    "\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    start_time = time.time()#一会记录日志要用\n",
    "\n",
    "    iter_per_epoch = len(train_loader)#看一下一次数据有多长，计算学习率要用\n",
    "\n",
    "    # --- 新增：使用tqdm包装DataLoader以显示进度条 ---\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{TrainArgs.epochs}\", leave=True)\n",
    "\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):#不断提取数据\n",
    "        X = X.to(TrainArgs.device)\n",
    "        Y = Y.to(TrainArgs.device)\n",
    "        loss_mask = loss_mask.to(TrainArgs.device)#数据上设备，上到gpu上\n",
    "\n",
    "        \n",
    "        lr = get_lr(epoch * iter_per_epoch + step, TrainArgs.epochs * iter_per_epoch, TrainArgs.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr                #计算当前学习率的算法\n",
    "\n",
    "        with TrainArgs.ctx:\n",
    "            res = model(X)#得到的是logits，aux_loss\n",
    "\n",
    "            loss = loss_fct(\n",
    "                res.logits.view(-1, res.logits.size(-1)),\n",
    "                Y.view(-1)\n",
    "            ).view(Y.size())  #计算loss的算法\n",
    "            loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "            loss += res.aux_loss\n",
    "            loss = loss / TrainArgs.accumulation_steps  \n",
    "\n",
    "            scaler.scale(loss).backward()  #反向传播\n",
    "\n",
    "        if (step + 1) % TrainArgs.accumulation_steps == 0:#判断梯度累加到位没有\n",
    "            scaler.unscale_(optimizer)#*前置操作\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), TrainArgs.grad_clip)\n",
    "            #梯度裁剪\n",
    "            scaler.step(optimizer)#参数更新\n",
    "            scaler.update()#*调整\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)#本次训练结束，清零梯度，为下一次反向传播准备\n",
    "\n",
    "        if step % TrainArgs.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            progress_bar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.3f}\",\n",
    "                \"lr\": f\"{current_lr:.2e}\"\n",
    "            })\n",
    "\n",
    "            if (swanlab is not None) : #用日志软件记录训练过程\n",
    "                swanlab.log({\"loss\": loss.item() * TrainArgs.accumulation_steps,\n",
    "                           \"lr\": optimizer.param_groups[-1]['lr'],\n",
    "                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n",
    "\n",
    "            # 保存checkpoint\n",
    "        if (step + 1) % TrainArgs.save_interval == 0:\n",
    "            # 只在更新步之后保存，确保梯度累积完成\n",
    "            if (step + 1) % TrainArgs.accumulation_steps == 0:\n",
    "                model.eval()\n",
    "                print(f\"\\nSaving checkpoint at step {step+1}...\")\n",
    "                \n",
    "                # 创建checkpoint字典\n",
    "                checkpoint = {\n",
    "                     # 对模型权重进行半精度转换\n",
    "                    'model': {k: v.half() for k, v in model.state_dict().items()},\n",
    "                    # 对优化器状态也进行转换（更严谨的做法）\n",
    "                    'optimizer': {\n",
    "                        'state': {k: v.half() if torch.is_tensor(v) else v for k, v in optimizer.state_dict()['state'].items()},\n",
    "                        'param_groups': optimizer.state_dict()['param_groups']\n",
    "                    },\n",
    "                    'scaler': scaler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'config': TrainArgs,\n",
    "                }\n",
    "                \n",
    "                # 保存\n",
    "                torch.save(checkpoint, TrainArgs.checkpoint_path)\n",
    "                print(\"Checkpoint saved.\")\n",
    "                model.train() # 切换回训练模式\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现循环训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正式训练前假如了这么一大堆逻辑，是为了这么个情节：\n",
    "训练了一小时，有事，还必须关电脑，\n",
    "那我们下次训练时就可以接着之前的chekpoint\n",
    "接着之前的模型权重、模型eopoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Resuming from checkpoint: ./output/latest_checkpoint.pth\n",
      "=> Resumed from epoch 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MiniMindForCausalLM(\n",
       "  (model): MiniMindModel(\n",
       "    (embed_tokens): Embedding(6400, 512)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x MiniMindBlock(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
       "          (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (mlp): MOEFeedForward(\n",
       "          (experts): ModuleList(\n",
       "            (0-3): 4 x FeedForward(\n",
       "              (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
       "              (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (gate): MoEGate()\n",
       "          (shared_experts): ModuleList(\n",
       "            (0): FeedForward(\n",
       "              (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
       "              (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
       "              (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=6400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # --- 关键：Checkpoint加载逻辑 ---\n",
    "start_epoch = 0\n",
    "start_step = 0\n",
    "if os.path.exists(TrainArgs.checkpoint_path):\n",
    "    print(f\"=> Resuming from checkpoint: {TrainArgs.checkpoint_path}\")\n",
    "    checkpoint = torch.load(TrainArgs.checkpoint_path, map_location=TrainArgs.device)\n",
    "    \n",
    "    # 加载模型权重 (处理DDP保存的权重)\n",
    "    model_state_dict = checkpoint['model']\n",
    "    if any(key.startswith('module.') for key in model_state_dict):\n",
    "            model_state_dict = {k.replace('module.', ''): v for k, v in model_state_dict.items()}\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    # 加载优化器和scaler状态\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scaler.load_state_dict(checkpoint['scaler'])\n",
    "    \n",
    "    # 恢复训练进度\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    # 注意：恢复step逻辑可以让dataloader跳过已训练数据，这里简化为从头开始当前epoch\n",
    "    print(f\"=> Resumed from epoch {start_epoch + 1}\")\n",
    "else:\n",
    "    print(\"=> Starting from scratch...\")\n",
    "\n",
    "model.to(TrainArgs.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练！ 外层epoch循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试训练运行以下代码\n",
    "for epoch in range(start_epoch, TrainArgs.epochs):\n",
    "        train_epoch(epoch, model, test_loader, optimizer, scaler, lm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5932650e345407d8df2856c77460718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/2:   0%|          | 0/176638 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#正式训练运行以下代码\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, TrainArgs\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 35\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, model, train_loader, optimizer, scaler, lm_config)\u001b[0m\n\u001b[1;32m     32\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39maux_loss\n\u001b[1;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m TrainArgs\u001b[38;5;241m.\u001b[39maccumulation_steps  \n\u001b[0;32m---> 35\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#反向传播\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m TrainArgs\u001b[38;5;241m.\u001b[39maccumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\u001b[38;5;66;03m#判断梯度累加到位没有\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\u001b[38;5;66;03m#*前置操作\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_proj/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_proj/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_proj/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#正式训练运行以下代码\n",
    "for epoch in range(start_epoch, TrainArgs.epochs):\n",
    "        train_epoch(epoch, model, train_loader, optimizer, scaler, lm_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
