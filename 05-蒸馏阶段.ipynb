{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from contextlib import nullcontext\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model_minimind import MiniMindConfig\n",
    "class TrainArgs():\n",
    "    #文件管理\n",
    "    out_dir = './DPO_output'\n",
    "    checkpoint_path = \"./DPO_output/latest_checkpoint.pth\"\n",
    "    data_path = '../data/dpo.jsonl'\n",
    "    #神经网络训练管理\n",
    "    epochs = 2\n",
    "    batch_size = 16\n",
    "    accumulation_steps = 4\n",
    "    learning_rate = 5e-4\n",
    "    warm_up = 0\n",
    "    grad_clip = 1\n",
    "    dtype = 'bfloat16'\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers = 0\n",
    "    log_interval = 50\n",
    "    save_interval = 2000\n",
    "    ctx = nullcontext() if device == \"cpu\" else torch.cuda.amp.autocast()\n",
    "class LLMargs():\n",
    "    use_moe = True\n",
    "    hidden_size = 512\n",
    "    num_hidden_layers = 8\n",
    "    \n",
    "\n",
    "lm_config = MiniMindConfig(use_moe=LLMargs.use_moe,hidden_size=LLMargs.hidden_size,num_hidden_layers=LLMargs.num_hidden_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型加载、数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained('../model/')\n",
    "student_model = MiniMindForCausalLM(lm_config).to(TrainArgs.device)\n",
    "#下面是蒸馏的特殊之处，还需要再加载个模型\n",
    "teacher_model = MiniMindForCausalLM(lm_config).to(TrainArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_teacher = './DPO_output/latest_checkpoint.pth'\n",
    "ckp_student = './sft_output/latest_checkpoint.pth'\n",
    "t_state_dict = torch.load(ckp_teacher, map_location=TrainArgs.device)\n",
    "s_state_dict = torch.load(ckp_student, map_location=TrainArgs.device)\n",
    "\n",
    "student_model.load_state_dict(s_state_dict, strict=False)\n",
    "teacher_model.load_state_dict(t_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.eval()\n",
    "teacher_model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import json\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self,  tokenizer,data_path, max_length=1024):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = self.load_data(data_path)\n",
    "        self.bos_id = tokenizer('<|im_start|>assistant', add_special_tokens=False).input_ids\n",
    "        self.eos_id = tokenizer('<|im_end|>', add_special_tokens=False).input_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def load_data(self, data_path):\n",
    "        samples = []\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                data = json.loads(line.strip())\n",
    "                samples.append(data)\n",
    "        return samples\n",
    "\n",
    "    def _create_chat_prompt(self, conversations):\n",
    "        \"\"\"构建符合ChatML格式的对话\"\"\"\n",
    "        messages = []\n",
    "        for i, turn in enumerate(conversations):\n",
    "            role = 'user' if i % 2 == 0 else 'assistant'\n",
    "            messages.append({\"role\": role, \"content\": turn['content']})\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "    def _generate_loss_mask(self, input_ids):\n",
    "        loss_mask = [0] * len(input_ids)\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            if input_ids[i:i + len(self.bos_id)] == self.bos_id:\n",
    "                start = i + len(self.bos_id)\n",
    "                end = start\n",
    "                while end < len(input_ids):\n",
    "                    if input_ids[end:end + len(self.eos_id)] == self.eos_id:\n",
    "                        break\n",
    "                    end += 1\n",
    "                for j in range(start + 1, min(end + len(self.eos_id) + 1, self.max_length)):\n",
    "                    loss_mask[j] = 1\n",
    "                i = end + len(self.eos_id) if end < len(input_ids) else len(input_ids)\n",
    "            else:\n",
    "                i += 1\n",
    "        return loss_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        # 构建对话提示\n",
    "        prompt = self._create_chat_prompt(sample['conversations'])\n",
    "        input_ids = self.tokenizer(prompt).input_ids[:self.max_length]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))\n",
    "\n",
    "        # 生成动态损失掩码\n",
    "        loss_mask = self._generate_loss_mask(input_ids)\n",
    "\n",
    "        # 构建训练数据\n",
    "        X = input_ids[:-1].clone().detach()\n",
    "        Y = input_ids[1:].clone().detach()\n",
    "        loss_mask = loss_mask[1:].clone().detach() # 对齐预测位置\n",
    "\n",
    "        return X, Y, loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds =SFTDataset(TrainArgs.data_path,tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{train_ds[1]}\\n数据集总长度为{len(train_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = Subset(train_ds,range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size = TrainArgs.batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = TrainArgs.num_workers,\n",
    "    pin_memory= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size = TrainArgs.batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = TrainArgs.num_workers,\n",
    "    pin_memory= True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import optim\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n",
    "optimizer = optim.AdamW(student_model.parameters(), lr=TrainArgs.learning_rate)\n",
    "#据模型在训练数据上的表现来调整模型的参数  训练的一个大脑\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(TrainArgs.dtype in ['float16', 'bfloat16']))\n",
    "#自动混合精度训练工具 训练的一个助手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swanlab\n",
    "# 初始化swanlab，传入项目名、实验名等\n",
    "swanlab.init(\n",
    "    project=\"MiniMind-Distillation\", \n",
    "    experiment_name=\"MiniMind-Distillation\", \n",
    "    config=vars(TrainArgs()) # 将你的所有超参数配置一次性传给swanlab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#蒸馏所需要的特殊loss算法\n",
    "def distillation_loss_fn(student_logits, teacher_logits, temperature=1.0, reduction='batchmean'):\n",
    "    with torch.no_grad():\n",
    "        teacher_probs = F.softmax(teacher_logits / temperature, hidden_size=-1).detach()\n",
    "\n",
    "    student_log_probs = F.log_softmax(student_logits / temperature, hidden_size=-1)\n",
    "\n",
    "    kl = F.kl_div(\n",
    "        student_log_probs,\n",
    "        teacher_probs,\n",
    "        reduction=reduction\n",
    "    )\n",
    "    return (temperature ** 2) * kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, train_loader, optimizer, scaler, lm_config,alpha=0.0, temperature=1.0):\n",
    "    #选择初始化损失函数！\n",
    "    model.train() # 确保模型处于训练模式\n",
    "\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    start_time = time.time()#一会记录日志要用\n",
    "\n",
    "    iter_per_epoch = len(train_loader)#看一下一次数据有多长，计算学习率要用\n",
    "\n",
    "    # --- 新增：使用tqdm包装DataLoader以显示进度条 ---\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{TrainArgs.epochs}\", leave=True)\n",
    "\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):#不断提取数据\n",
    "        X = X.to(TrainArgs.device)\n",
    "        Y = Y.to(TrainArgs.device)\n",
    "        loss_mask = loss_mask.to(TrainArgs.device)#数据上设备，上到gpu上\n",
    "\n",
    "        \n",
    "        lr = get_lr(epoch * iter_per_epoch + step, TrainArgs.epochs * iter_per_epoch, TrainArgs.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr                #计算当前学习率的算法\n",
    "\n",
    "        with TrainArgs.ctx:\n",
    "            res = model(X)\n",
    "            student_logits = res.logits\n",
    "\n",
    "            # 教师模型前向传播（只在eval & no_grad）\n",
    "            if teacher_model is not None:\n",
    "                with torch.no_grad():\n",
    "                    teacher_logits = teacher_model(X).logits\n",
    "                    vocab_size_student = student_logits.size(-1)  # N\n",
    "                    teacher_logits = teacher_logits[..., :vocab_size_student]\n",
    "\n",
    "\n",
    "            # ========== 计算损失 ==========\n",
    "        # 1) Ground-Truth CE Loss（可选）\n",
    "        loss_mask_flat = loss_mask.view(-1)\n",
    "        ce_loss = F.cross_entropy(\n",
    "            student_logits.view(-1, student_logits.size(-1)),\n",
    "            Y.view(-1),\n",
    "            ignore_index=0,\n",
    "            reduction='none'\n",
    "        )\n",
    "        ce_loss = torch.sum(ce_loss * loss_mask_flat) / loss_mask_flat.sum()\n",
    "        if lm_config.use_moe:\n",
    "            ce_loss += res.aux_loss\n",
    "\n",
    "        # 2) Distillation Loss（可选）\n",
    "        if teacher_model is not None:\n",
    "            # 只在有效token位置做蒸馏\n",
    "            distill_loss = distillation_loss_fn(\n",
    "                student_logits.view(-1, student_logits.size(-1))[loss_mask_flat == 1],\n",
    "                teacher_logits.view(-1, teacher_logits.size(-1))[loss_mask_flat == 1],\n",
    "                temperature=temperature\n",
    "            )\n",
    "        else:\n",
    "            distill_loss = torch.tensor(0.0, device=args.device)\n",
    "\n",
    "        # 3) 总损失 = alpha * CE + (1-alpha) * Distill\n",
    "        loss = (alpha * ce_loss + (1 - alpha) * distill_loss) / args.accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()  #反向传播\n",
    "\n",
    "        if (step + 1) % TrainArgs.accumulation_steps == 0:#判断梯度累加到位没有\n",
    "            scaler.unscale_(optimizer)#*前置操作\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), TrainArgs.grad_clip)\n",
    "            #梯度裁剪\n",
    "            scaler.step(optimizer)#参数更新\n",
    "            scaler.update()#*调整\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)#本次训练结束，清零梯度，为下一次反向传播准备\n",
    "\n",
    "        if step % TrainArgs.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            progress_bar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.3f}\",\n",
    "                \"lr\": f\"{current_lr:.2e}\"\n",
    "            })\n",
    "\n",
    "            if (swanlab is not None) : #用日志软件记录训练过程\n",
    "                swanlab.log({\"loss\": loss.item() * TrainArgs.accumulation_steps,\n",
    "                           \"lr\": optimizer.param_groups[-1]['lr'],\n",
    "                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n",
    "\n",
    "            # 保存checkpoint\n",
    "        if (step + 1) % TrainArgs.save_interval == 0:\n",
    "            # 只在更新步之后保存，确保梯度累积完成\n",
    "            if (step + 1) % TrainArgs.accumulation_steps == 0:\n",
    "                model.eval()\n",
    "                print(f\"\\nSaving checkpoint at step {step+1}...\")\n",
    "                \n",
    "                # 创建checkpoint字典\n",
    "                checkpoint = {\n",
    "                     # 对模型权重进行半精度转换\n",
    "                    'model': {k: v.half() for k, v in model.state_dict().items()},\n",
    "                    # 对优化器状态也进行转换（更严谨的做法）\n",
    "                    'optimizer': {\n",
    "                        'state': {k: v.half() if torch.is_tensor(v) else v for k, v in optimizer.state_dict()['state'].items()},\n",
    "                        'param_groups': optimizer.state_dict()['param_groups']\n",
    "                    },\n",
    "                    'scaler': scaler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'config': TrainArgs,\n",
    "                }\n",
    "                \n",
    "                # 保存\n",
    "                torch.save(checkpoint, TrainArgs.checkpoint_path)\n",
    "                print(\"Checkpoint saved.\")\n",
    "                model.train() # 切换回训练模式\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
