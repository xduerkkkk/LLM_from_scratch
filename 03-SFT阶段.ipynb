{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境与设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "Number of GPUs: 1\n",
      "  GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "    Memory: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model_minimind import MiniMindConfig\n",
    "class TrainArgs():\n",
    "    #文件管理\n",
    "    out_dir = './sft_output'\n",
    "    checkpoint_path = \"./sft_output/latest_checkpoint.pth\"\n",
    "    data_path = '../data/sft_1024.jsonl'\n",
    "    #神经网络训练管理\n",
    "    epochs = 2\n",
    "    batch_size = 16\n",
    "    accumulation_steps = 4\n",
    "    learning_rate = 5e-4\n",
    "    warm_up = 0\n",
    "    grad_clip = 1\n",
    "    dtype = 'bfloat16'\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers = 0\n",
    "    log_interval = 50\n",
    "    save_interval = 2000\n",
    "    ctx = nullcontext() if device == \"cpu\" else torch.cuda.amp.autocast()\n",
    "class LLMargs():\n",
    "    use_moe = True\n",
    "    hidden_size = 512\n",
    "    num_hidden_layers = 8\n",
    "    \n",
    "\n",
    "lm_config = MiniMindConfig(use_moe=LLMargs.use_moe,hidden_size=LLMargs.hidden_size,num_hidden_layers=LLMargs.num_hidden_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载模型与数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained('../model/')\n",
    "model = MiniMindForCausalLM(lm_config).to(TrainArgs.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.mlp.experts.0.gate_proj.weight', 'model.layers.0.mlp.experts.0.down_proj.weight', 'model.layers.0.mlp.experts.0.up_proj.weight', 'model.layers.0.mlp.experts.1.gate_proj.weight', 'model.layers.0.mlp.experts.1.down_proj.weight', 'model.layers.0.mlp.experts.1.up_proj.weight', 'model.layers.0.mlp.experts.2.gate_proj.weight', 'model.layers.0.mlp.experts.2.down_proj.weight', 'model.layers.0.mlp.experts.2.up_proj.weight', 'model.layers.0.mlp.experts.3.gate_proj.weight', 'model.layers.0.mlp.experts.3.down_proj.weight', 'model.layers.0.mlp.experts.3.up_proj.weight', 'model.layers.0.mlp.gate.weight', 'model.layers.0.mlp.shared_experts.0.gate_proj.weight', 'model.layers.0.mlp.shared_experts.0.down_proj.weight', 'model.layers.0.mlp.shared_experts.0.up_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.mlp.experts.0.gate_proj.weight', 'model.layers.1.mlp.experts.0.down_proj.weight', 'model.layers.1.mlp.experts.0.up_proj.weight', 'model.layers.1.mlp.experts.1.gate_proj.weight', 'model.layers.1.mlp.experts.1.down_proj.weight', 'model.layers.1.mlp.experts.1.up_proj.weight', 'model.layers.1.mlp.experts.2.gate_proj.weight', 'model.layers.1.mlp.experts.2.down_proj.weight', 'model.layers.1.mlp.experts.2.up_proj.weight', 'model.layers.1.mlp.experts.3.gate_proj.weight', 'model.layers.1.mlp.experts.3.down_proj.weight', 'model.layers.1.mlp.experts.3.up_proj.weight', 'model.layers.1.mlp.gate.weight', 'model.layers.1.mlp.shared_experts.0.gate_proj.weight', 'model.layers.1.mlp.shared_experts.0.down_proj.weight', 'model.layers.1.mlp.shared_experts.0.up_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.mlp.experts.0.gate_proj.weight', 'model.layers.2.mlp.experts.0.down_proj.weight', 'model.layers.2.mlp.experts.0.up_proj.weight', 'model.layers.2.mlp.experts.1.gate_proj.weight', 'model.layers.2.mlp.experts.1.down_proj.weight', 'model.layers.2.mlp.experts.1.up_proj.weight', 'model.layers.2.mlp.experts.2.gate_proj.weight', 'model.layers.2.mlp.experts.2.down_proj.weight', 'model.layers.2.mlp.experts.2.up_proj.weight', 'model.layers.2.mlp.experts.3.gate_proj.weight', 'model.layers.2.mlp.experts.3.down_proj.weight', 'model.layers.2.mlp.experts.3.up_proj.weight', 'model.layers.2.mlp.gate.weight', 'model.layers.2.mlp.shared_experts.0.gate_proj.weight', 'model.layers.2.mlp.shared_experts.0.down_proj.weight', 'model.layers.2.mlp.shared_experts.0.up_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.mlp.experts.0.gate_proj.weight', 'model.layers.3.mlp.experts.0.down_proj.weight', 'model.layers.3.mlp.experts.0.up_proj.weight', 'model.layers.3.mlp.experts.1.gate_proj.weight', 'model.layers.3.mlp.experts.1.down_proj.weight', 'model.layers.3.mlp.experts.1.up_proj.weight', 'model.layers.3.mlp.experts.2.gate_proj.weight', 'model.layers.3.mlp.experts.2.down_proj.weight', 'model.layers.3.mlp.experts.2.up_proj.weight', 'model.layers.3.mlp.experts.3.gate_proj.weight', 'model.layers.3.mlp.experts.3.down_proj.weight', 'model.layers.3.mlp.experts.3.up_proj.weight', 'model.layers.3.mlp.gate.weight', 'model.layers.3.mlp.shared_experts.0.gate_proj.weight', 'model.layers.3.mlp.shared_experts.0.down_proj.weight', 'model.layers.3.mlp.shared_experts.0.up_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.mlp.experts.0.gate_proj.weight', 'model.layers.4.mlp.experts.0.down_proj.weight', 'model.layers.4.mlp.experts.0.up_proj.weight', 'model.layers.4.mlp.experts.1.gate_proj.weight', 'model.layers.4.mlp.experts.1.down_proj.weight', 'model.layers.4.mlp.experts.1.up_proj.weight', 'model.layers.4.mlp.experts.2.gate_proj.weight', 'model.layers.4.mlp.experts.2.down_proj.weight', 'model.layers.4.mlp.experts.2.up_proj.weight', 'model.layers.4.mlp.experts.3.gate_proj.weight', 'model.layers.4.mlp.experts.3.down_proj.weight', 'model.layers.4.mlp.experts.3.up_proj.weight', 'model.layers.4.mlp.gate.weight', 'model.layers.4.mlp.shared_experts.0.gate_proj.weight', 'model.layers.4.mlp.shared_experts.0.down_proj.weight', 'model.layers.4.mlp.shared_experts.0.up_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.mlp.experts.0.gate_proj.weight', 'model.layers.5.mlp.experts.0.down_proj.weight', 'model.layers.5.mlp.experts.0.up_proj.weight', 'model.layers.5.mlp.experts.1.gate_proj.weight', 'model.layers.5.mlp.experts.1.down_proj.weight', 'model.layers.5.mlp.experts.1.up_proj.weight', 'model.layers.5.mlp.experts.2.gate_proj.weight', 'model.layers.5.mlp.experts.2.down_proj.weight', 'model.layers.5.mlp.experts.2.up_proj.weight', 'model.layers.5.mlp.experts.3.gate_proj.weight', 'model.layers.5.mlp.experts.3.down_proj.weight', 'model.layers.5.mlp.experts.3.up_proj.weight', 'model.layers.5.mlp.gate.weight', 'model.layers.5.mlp.shared_experts.0.gate_proj.weight', 'model.layers.5.mlp.shared_experts.0.down_proj.weight', 'model.layers.5.mlp.shared_experts.0.up_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.mlp.experts.0.gate_proj.weight', 'model.layers.6.mlp.experts.0.down_proj.weight', 'model.layers.6.mlp.experts.0.up_proj.weight', 'model.layers.6.mlp.experts.1.gate_proj.weight', 'model.layers.6.mlp.experts.1.down_proj.weight', 'model.layers.6.mlp.experts.1.up_proj.weight', 'model.layers.6.mlp.experts.2.gate_proj.weight', 'model.layers.6.mlp.experts.2.down_proj.weight', 'model.layers.6.mlp.experts.2.up_proj.weight', 'model.layers.6.mlp.experts.3.gate_proj.weight', 'model.layers.6.mlp.experts.3.down_proj.weight', 'model.layers.6.mlp.experts.3.up_proj.weight', 'model.layers.6.mlp.gate.weight', 'model.layers.6.mlp.shared_experts.0.gate_proj.weight', 'model.layers.6.mlp.shared_experts.0.down_proj.weight', 'model.layers.6.mlp.shared_experts.0.up_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.mlp.experts.0.gate_proj.weight', 'model.layers.7.mlp.experts.0.down_proj.weight', 'model.layers.7.mlp.experts.0.up_proj.weight', 'model.layers.7.mlp.experts.1.gate_proj.weight', 'model.layers.7.mlp.experts.1.down_proj.weight', 'model.layers.7.mlp.experts.1.up_proj.weight', 'model.layers.7.mlp.experts.2.gate_proj.weight', 'model.layers.7.mlp.experts.2.down_proj.weight', 'model.layers.7.mlp.experts.2.up_proj.weight', 'model.layers.7.mlp.experts.3.gate_proj.weight', 'model.layers.7.mlp.experts.3.down_proj.weight', 'model.layers.7.mlp.experts.3.up_proj.weight', 'model.layers.7.mlp.gate.weight', 'model.layers.7.mlp.shared_experts.0.gate_proj.weight', 'model.layers.7.mlp.shared_experts.0.down_proj.weight', 'model.layers.7.mlp.shared_experts.0.up_proj.weight', 'model.norm.weight', 'lm_head.weight'], unexpected_keys=['optimizer', 'scaler', 'epoch', 'step', 'config'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ckp = './pretrain_output/latest_checkpoint.pth'\n",
    "state_dict = torch.load(ckp, map_location=TrainArgs.device)\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniMindForCausalLM(\n",
      "  (model): MiniMindModel(\n",
      "    (embed_tokens): Embedding(6400, 512)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x MiniMindBlock(\n",
      "        (self_attn): Attention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "        (mlp): MOEFeedForward(\n",
      "          (experts): ModuleList(\n",
      "            (0-3): 4 x FeedForward(\n",
      "              (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "              (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (gate): MoEGate()\n",
      "          (shared_experts): ModuleList(\n",
      "            (0): FeedForward(\n",
      "              (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "              (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "              (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=6400, bias=False)\n",
      ")\n",
      "模型总参数量 (Total Parameters): 95,052,288\n",
      "可训练参数量 (Trainable Parameters): 95,052,288\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# 打印结果，使用 f-string 的格式化功能让数字更易读（例如，加上千位分隔符）\n",
    "print(f\"模型总参数量 (Total Parameters): {total_params:,}\")\n",
    "\n",
    "# 如果你还想区分“可训练参数”，可以这样做：\n",
    "# 在大多数情况下，总参数量和可训练参数量是一样的。\n",
    "# 除非你手动设置了某些参数的 requires_grad=False (冻结了某些层)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"可训练参数量 (Trainable Parameters): {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import json\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self,  tokenizer,data_path, max_length=1024):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = self.load_data(data_path)\n",
    "        self.bos_id = tokenizer('<|im_start|>assistant', add_special_tokens=False).input_ids\n",
    "        self.eos_id = tokenizer('<|im_end|>', add_special_tokens=False).input_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def load_data(self, data_path):\n",
    "        samples = []\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                data = json.loads(line.strip())\n",
    "                samples.append(data)\n",
    "        return samples\n",
    "\n",
    "    def _create_chat_prompt(self, conversations):\n",
    "        \"\"\"构建符合ChatML格式的对话\"\"\"\n",
    "        messages = []\n",
    "        for i, turn in enumerate(conversations):\n",
    "            role = 'user' if i % 2 == 0 else 'assistant'\n",
    "            messages.append({\"role\": role, \"content\": turn['content']})\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "    def _generate_loss_mask(self, input_ids):\n",
    "        loss_mask = [0] * len(input_ids)\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            if input_ids[i:i + len(self.bos_id)] == self.bos_id:\n",
    "                start = i + len(self.bos_id)\n",
    "                end = start\n",
    "                while end < len(input_ids):\n",
    "                    if input_ids[end:end + len(self.eos_id)] == self.eos_id:\n",
    "                        break\n",
    "                    end += 1\n",
    "                for j in range(start + 1, min(end + len(self.eos_id) + 1, self.max_length)):\n",
    "                    loss_mask[j] = 1\n",
    "                i = end + len(self.eos_id) if end < len(input_ids) else len(input_ids)\n",
    "            else:\n",
    "                i += 1\n",
    "        return loss_mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        # 构建对话提示\n",
    "        prompt = self._create_chat_prompt(sample['conversations'])\n",
    "        input_ids = self.tokenizer(prompt).input_ids[:self.max_length]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))\n",
    "\n",
    "        # 生成动态损失掩码\n",
    "        loss_mask = self._generate_loss_mask(input_ids)\n",
    "\n",
    "        # 构建训练数据\n",
    "        X = input_ids[:-1].clone().detach()\n",
    "        Y = input_ids[1:].clone().detach()\n",
    "        loss_mask = loss_mask[1:].clone().detach() # 对齐预测位置\n",
    "\n",
    "        return X, Y, loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SFTDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mSFTDataset\u001b[49m(tokenizer,TrainArgs\u001b[38;5;241m.\u001b[39mdata_path,max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SFTDataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_ds = SFTDataset(tokenizer,TrainArgs.data_path,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_ds[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m数据集总长度为\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'{train_ds[1]}\\n数据集总长度为{len(train_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = Subset(train_ds,range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size = TrainArgs.batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = TrainArgs.num_workers,\n",
    "    pin_memory= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size = TrainArgs.batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = TrainArgs.num_workers,\n",
    "    pin_memory= True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = False\n",
    "for batch_idx, (X_batch, Y_batch, loss_mask_batch) in enumerate(train_loader):\n",
    "    if not tag:\n",
    "        print(f\"Batch {batch_idx}:\")\n",
    "        print(f\"  X_batch shape: {X_batch.shape}\")\n",
    "        print(f\"  Y_batch shape: {Y_batch.shape}\")\n",
    "        print(f\"  loss_mask_batch shape: {loss_mask_batch.shape}\")\n",
    "        tag = True\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = False\n",
    "for batch_idx, (X_batch, Y_batch, loss_mask_batch) in enumerate(test_loader):\n",
    "    if not tag:\n",
    "        print(f\"Batch {batch_idx}:\")\n",
    "        print(f\"  X_batch shape: {X_batch.shape}\")\n",
    "        print(f\"  Y_batch shape: {Y_batch.shape}\")\n",
    "        print(f\"  loss_mask_batch shape: {loss_mask_batch.shape}\")\n",
    "        tag = True\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import optim\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n",
    "optimizer = optim.AdamW(model.parameters(), lr=TrainArgs.learning_rate)\n",
    "#据模型在训练数据上的表现来调整模型的参数  训练的一个大脑\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(TrainArgs.dtype in ['float16', 'bfloat16']))\n",
    "#自动混合精度训练工具 训练的一个助手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swanlab\n",
    "# 初始化swanlab，传入项目名、实验名等\n",
    "swanlab.init(\n",
    "    project=\"MiniMind-SFT\", \n",
    "    experiment_name=\"MiniMind-SFT\", \n",
    "    config=vars(TrainArgs()) # 将你的所有超参数配置一次性传给swanlab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, train_loader, optimizer, scaler, lm_config):\n",
    "    #选择初始化损失函数！\n",
    "    model.train() # 确保模型处于训练模式\n",
    "\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    start_time = time.time()#一会记录日志要用\n",
    "\n",
    "    iter_per_epoch = len(train_loader)#看一下一次数据有多长，计算学习率要用\n",
    "\n",
    "    # --- 新增：使用tqdm包装DataLoader以显示进度条 ---\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{TrainArgs.epochs}\", leave=True)\n",
    "\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):#不断提取数据\n",
    "        X = X.to(TrainArgs.device)\n",
    "        Y = Y.to(TrainArgs.device)\n",
    "        loss_mask = loss_mask.to(TrainArgs.device)#数据上设备，上到gpu上\n",
    "\n",
    "        \n",
    "        lr = get_lr(epoch * iter_per_epoch + step, TrainArgs.epochs * iter_per_epoch, TrainArgs.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr                #计算当前学习率的算法\n",
    "\n",
    "        with TrainArgs.ctx:\n",
    "            res = model(X)#得到的是logits，aux_loss\n",
    "\n",
    "            loss = loss_fct(\n",
    "                res.logits.view(-1, res.logits.size(-1)),\n",
    "                Y.view(-1)\n",
    "            ).view(Y.size())  #计算loss的算法\n",
    "            loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "            loss += res.aux_loss\n",
    "            loss = loss / TrainArgs.accumulation_steps  \n",
    "\n",
    "            scaler.scale(loss).backward()  #反向传播\n",
    "\n",
    "        if (step + 1) % TrainArgs.accumulation_steps == 0:#判断梯度累加到位没有\n",
    "            scaler.unscale_(optimizer)#*前置操作\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), TrainArgs.grad_clip)\n",
    "            #梯度裁剪\n",
    "            scaler.step(optimizer)#参数更新\n",
    "            scaler.update()#*调整\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)#本次训练结束，清零梯度，为下一次反向传播准备\n",
    "\n",
    "        if step % TrainArgs.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            progress_bar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.3f}\",\n",
    "                \"lr\": f\"{current_lr:.2e}\"\n",
    "            })\n",
    "\n",
    "            if (swanlab is not None) : #用日志软件记录训练过程\n",
    "                swanlab.log({\"loss\": loss.item() * TrainArgs.accumulation_steps,\n",
    "                           \"lr\": optimizer.param_groups[-1]['lr'],\n",
    "                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n",
    "\n",
    "            # 保存checkpoint\n",
    "        if (step + 1) % TrainArgs.save_interval == 0:\n",
    "            # 只在更新步之后保存，确保梯度累积完成\n",
    "            if (step + 1) % TrainArgs.accumulation_steps == 0:\n",
    "                model.eval()\n",
    "                print(f\"\\nSaving checkpoint at step {step+1}...\")\n",
    "                \n",
    "                # 创建checkpoint字典\n",
    "                checkpoint = {\n",
    "                     # 对模型权重进行半精度转换\n",
    "                    'model': {k: v.half() for k, v in model.state_dict().items()},\n",
    "                    # 对优化器状态也进行转换（更严谨的做法）\n",
    "                    'optimizer': {\n",
    "                        'state': {k: v.half() if torch.is_tensor(v) else v for k, v in optimizer.state_dict()['state'].items()},\n",
    "                        'param_groups': optimizer.state_dict()['param_groups']\n",
    "                    },\n",
    "                    'scaler': scaler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'config': TrainArgs,\n",
    "                }\n",
    "                \n",
    "                # 保存\n",
    "                torch.save(checkpoint, TrainArgs.checkpoint_path)\n",
    "                print(\"Checkpoint saved.\")\n",
    "                model.train() # 切换回训练模式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现循环训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 关键：Checkpoint加载逻辑 ---\n",
    "start_epoch = 0\n",
    "start_step = 0\n",
    "if os.path.exists(TrainArgs.checkpoint_path):\n",
    "    print(f\"=> Resuming from checkpoint: {TrainArgs.checkpoint_path}\")\n",
    "    checkpoint = torch.load(TrainArgs.checkpoint_path, map_location=TrainArgs.device)\n",
    "    \n",
    "    # 加载模型权重 (处理DDP保存的权重)\n",
    "    model_state_dict = checkpoint['model']\n",
    "    if any(key.startswith('module.') for key in model_state_dict):\n",
    "            model_state_dict = {k.replace('module.', ''): v for k, v in model_state_dict.items()}\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    # 加载优化器和scaler状态\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scaler.load_state_dict(checkpoint['scaler'])\n",
    "    \n",
    "    # 恢复训练进度\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    # 注意：恢复step逻辑可以让dataloader跳过已训练数据，这里简化为从头开始当前epoch\n",
    "    print(f\"=> Resumed from epoch {start_epoch + 1}\")\n",
    "else:\n",
    "    print(\"=> Starting from scratch...\")\n",
    "\n",
    "model.to(TrainArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试训练运行以下代码\n",
    "for epoch in range(start_epoch, TrainArgs.epochs):\n",
    "        train_epoch(epoch, model, test_loader, optimizer, scaler, lm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正式训练运行以下代码\n",
    "for epoch in range(start_epoch, TrainArgs.epochs):\n",
    "        train_epoch(epoch, model, train_loader, optimizer, scaler, lm_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
