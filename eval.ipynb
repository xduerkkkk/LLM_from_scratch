{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30403/2837662099.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx = nullcontext() if device == \"cpu\" else torch.cuda.amp.autocast()\n"
     ]
    }
   ],
   "source": [
    "class TrainArgs():\n",
    "    #æ–‡ä»¶ç®¡ç†\n",
    "    out_dir = './pretrain_output'\n",
    "    checkpoint_path = \"./pretrain_output/latest_checkpoint.pth\"\n",
    "    data_path = 'data/pretrain_hq.jsonl'\n",
    "    #ç¥ç»ç½‘ç»œè®­ç»ƒç®¡ç†\n",
    "    epochs = 2\n",
    "    batch_size = 32\n",
    "    accumulation_steps = 4\n",
    "    learning_rate = 5e-4\n",
    "    warm_up = 0\n",
    "    grad_clip = 1\n",
    "    dtype = 'bfloat16'\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers = min(8, os.cpu_count()) \n",
    "    log_interval = 50\n",
    "    save_interval = 2000\n",
    "    ctx = nullcontext() if device == \"cpu\" else torch.cuda.amp.autocast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalArgs():\n",
    "    hidden_size = 512\n",
    "    temperature = 0.85\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    num_hidden_layers = 8\n",
    "    history_cnt = 0\n",
    "    model_mode = 0 #0: é¢„è®­ç»ƒæ¨¡å‹ï¼Œ1: SFT-Chatæ¨¡å‹ï¼Œ2: RLHF-Chatæ¨¡å‹ï¼Œ3: Reasonæ¨¡å‹ï¼Œ4: RLAIF-Chatæ¨¡å‹\"\n",
    "    max_seq_len = 8192\n",
    "    top_p =0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆå§‹åŒ–æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(args):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./model/')\n",
    "    \n",
    "        \n",
    "    modes = {0: 'pretrain', 1: 'full_sft', 2: 'rlhf', 3: 'reason', 4: 'grpo'}\n",
    "    ckp = f'{modes[args.model_mode]}_output/latest_checkpoint.pth'\n",
    "\n",
    "    model = MiniMindForCausalLM(MiniMindConfig(\n",
    "        hidden_size=args.hidden_size,\n",
    "        num_hidden_layers=args.num_hidden_layers,\n",
    "        \n",
    "    ))\n",
    "\n",
    "    # 1. åŠ è½½å®Œæ•´çš„ checkpoint å­—å…¸ï¼Œå¹¶æŒ‡æ˜ weights_only=False\n",
    "    print(f\"Loading checkpoint from {ckp}...\")\n",
    "    checkpoint = torch.load(ckp, map_location=args.device, weights_only=False)\n",
    "    \n",
    "    # 2. ä»å­—å…¸ä¸­æå–æ¨¡å‹çš„ state_dict æ¥åŠ è½½\n",
    "    model.load_state_dict(checkpoint['model'], strict=True)\n",
    "    \n",
    "    print(\"Checkpoint loaded successfully.\")\n",
    "    return model.eval().to(args.device), tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_datas(args):\n",
    "    if args.model_mode == 0:\n",
    "        # pretrainæ¨¡å‹çš„æ¥é¾™èƒ½åŠ›ï¼ˆæ— æ³•å¯¹è¯ï¼‰\n",
    "        prompt_datas = [\n",
    "            'é©¬å…‹æ€ä¸»ä¹‰åŸºæœ¬åŸç†',\n",
    "            'äººç±»å¤§è„‘çš„ä¸»è¦åŠŸèƒ½',\n",
    "            'ä¸‡æœ‰å¼•åŠ›åŸç†æ˜¯',\n",
    "            'ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯',\n",
    "            'äºŒæ°§åŒ–ç¢³åœ¨ç©ºæ°”ä¸­',\n",
    "            'åœ°çƒä¸Šæœ€å¤§çš„åŠ¨ç‰©æœ‰',\n",
    "            'æ­å·å¸‚çš„ç¾é£Ÿæœ‰'\n",
    "        ]\n",
    "    else:\n",
    "       \n",
    "        # é€šç”¨å¯¹è¯é—®é¢˜\n",
    "        prompt_datas = [\n",
    "            'è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚',\n",
    "            'ä½ æ›´æ“…é•¿å“ªä¸€ä¸ªå­¦ç§‘ï¼Ÿ',\n",
    "            'é²è¿…çš„ã€Šç‹‚äººæ—¥è®°ã€‹æ˜¯å¦‚ä½•æ‰¹åˆ¤å°å»ºç¤¼æ•™çš„ï¼Ÿ',\n",
    "            'æˆ‘å’³å—½å·²ç»æŒç»­äº†ä¸¤å‘¨ï¼Œéœ€è¦å»åŒ»é™¢æ£€æŸ¥å—ï¼Ÿ',\n",
    "            'è¯¦ç»†çš„ä»‹ç»å…‰é€Ÿçš„ç‰©ç†æ¦‚å¿µã€‚',\n",
    "            'æ¨èä¸€äº›æ­å·çš„ç‰¹è‰²ç¾é£Ÿå§ã€‚',\n",
    "            'è¯·ä¸ºæˆ‘è®²è§£â€œå¤§è¯­è¨€æ¨¡å‹â€è¿™ä¸ªæ¦‚å¿µã€‚',\n",
    "            'å¦‚ä½•ç†è§£ChatGPTï¼Ÿ',\n",
    "            'Introduce the history of the United States, please.'\n",
    "        ]\n",
    "        \n",
    "            \n",
    "\n",
    "    return prompt_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­£å¼è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from pretrain_output/latest_checkpoint.pth...\n",
      "Checkpoint loaded successfully.\n",
      "ğŸ‘¶: é©¬å…‹æ€ä¸»ä¹‰åŸºæœ¬åŸç†\n",
      "ğŸ¤–ï¸: æ˜¯ä»€ä¹ˆï¼Ÿé©¬å…‹æ€ä¸»ä¹‰åŸºæœ¬åŸç†æ˜¯å°†äººç±»æ™ºæ…§å’Œåˆ›é€ åŠ›é›†ä¸­åœ¨äººç±»çš„æ€ç»´èƒ½åŠ›ä¹‹å¤–ï¼Œå³é€šè¿‡ç§‘å­¦æ–¹æ³•å’Œæ€ç»´æ‰‹æ®µï¼Œåœ¨ä¸åŒçš„å†å²æ—¶æœŸæˆ–æ–‡åŒ–èƒŒæ™¯ä¸‹ï¼Œå°†äººç±»æ™ºæ…§ä¸åˆ›é€ åŠ›ç›¸ç»“åˆï¼Œä»è€Œå®ç°äººç±»æ–‡æ˜çš„è¿›æ­¥å’Œå‘å±•ã€‚\n",
      "\n",
      "\n",
      "\n",
      "ğŸ‘¶: äººç±»å¤§è„‘çš„ä¸»è¦åŠŸèƒ½\n",
      "ğŸ¤–ï¸: æ˜¯ä»€ä¹ˆï¼Ÿ äººç±»å¤§è„‘ä¸»è¦åŠŸèƒ½æ˜¯é€šè¿‡ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥æ¥è¿›è¡Œä¿¡æ¯å¤„ç†å’Œä¿¡æ¯ä¼ é€’ã€‚åœ¨å¤§è„‘ä¸­ï¼Œç¥ç»å…ƒé€šè¿‡ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥äº§ç”Ÿä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯ä¼ é€’ç»™å¤§è„‘ã€‚ç¥ç»å…ƒé€šè¿‡è¿™äº›ç¥ç»å…ƒè¿æ¥æˆç¥ç»å…ƒï¼Œè¿™äº›ç¥ç»å…ƒä¹‹é—´é€šè¿‡ç”µä¿¡å·ä¼ é€’ä¿¡æ¯ã€‚ç¥ç»å…ƒæ˜¯ç¥ç»ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå®ƒä»¬é€šè¿‡ç¥ç»å…ƒä¹‹é—´çš„ä¿¡å·ä¼ é€’ï¼Œå¹¶ä¼ é€’åˆ°å¤§è„‘çš„å„ä¸ªéƒ¨ä½ã€‚ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥å¯ä»¥æ˜¯æ„ŸçŸ¥ã€è¿åŠ¨ã€å­¦ä¹ ã€è®°å¿†ç­‰å¤šç§åŠŸèƒ½ï¼Œç¥ç»å…ƒä¹‹é—´çš„ç›¸äº’ä½œç”¨å¯ä»¥å½±å“è®¤çŸ¥å’Œè¡Œä¸ºã€‚\n",
      "\n",
      "\n",
      "\n",
      "ğŸ‘¶: ä¸‡æœ‰å¼•åŠ›åŸç†æ˜¯\n",
      "ğŸ¤–ï¸: å¦‚ä½•è¿ä½œçš„ï¼Ÿä¸‡æœ‰å¼•åŠ›åŸç†æ˜¯é€šè¿‡å¼•åŠ›ä½œç”¨åŠ›æ¥æè¿°çš„ã€‚\n",
      "\n",
      "å¼•åŠ›æ˜¯ç”±è´¨é‡å’Œè·ç¦»ä¹‹é—´äº§ç”Ÿçš„ï¼Œè¿™äº›è·ç¦»ä¸è·ç¦»çš„å¹³æ–¹æˆåæ¯”ã€‚\n",
      "\n",
      "å½“ä¸¤ä¸ªç‰©ä½“ä¹‹é—´å­˜åœ¨å¼•åŠ›æ—¶ï¼Œå®ƒä»¬ä¹‹é—´çš„å¼•åŠ›ä¼šå‘ç”Ÿå˜åŒ–ï¼Œè¿™ä¸ªå˜åŒ–çš„ç»“æœå°±æ˜¯å®ƒä»¬ä¹‹é—´ç›¸äº’å¸å¼•çš„å…³ç³»ã€‚å¼•åŠ›ä¼šå°†ç‰©ä½“å¸å¼•åˆ°å®ƒä»¬çš„è½¨é“ï¼Œè¿™ä¼šå½±å“å®ƒä»¬ä¹‹é—´çš„å¼•åŠ›ã€‚\n",
      "\n",
      "å½“ä¸¤ä¸ªç‰©ä½“ä¹‹é—´å­˜åœ¨å¼•åŠ›æ—¶ï¼Œå®ƒä»¬çš„å¼•åŠ›ä¼šå‘ç”Ÿå˜åŒ–ï¼Œä¾‹å¦‚ï¼Œå½“ä¸¤ä¸ªç‰©ä½“å—åˆ°ä¸€ä¸ªåŠ›æ—¶ï¼Œå®ƒä»¬ä¹‹é—´çš„å¼•åŠ›ä¼šæ”¹å˜å®ƒä»¬ä¹‹é—´çš„è·ç¦»ã€‚å¦‚æœä¸¤ä¸ªç‰©ä½“ä¹‹é—´çš„å¼•åŠ›ä¸åŒï¼Œå®ƒä»¬ä¹‹é—´çš„å¼•åŠ›ä¹Ÿä¼šäº§ç”Ÿå˜åŒ–ï¼Œä¾‹å¦‚ï¼Œå½“ä¸¤ä¸ªç‰©ä½“çš„è´¨é‡å¢åŠ æ—¶ï¼Œå®ƒä»¬ä¹‹é—´çš„å¼•åŠ›ä¼šå‡å°‘ã€‚\n",
      "\n",
      "\n",
      "\n",
      "ğŸ‘¶: ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯\n",
      "ğŸ¤–ï¸: ç ç©†æœ—ç›å³°ï¼Œæµ·æ‹”8,848ç±³ã€‚å®ƒä½äºå–œé©¬æ‹‰é›…å±±è„‰ä¸­ï¼Œè¢«ç§°ä¸ºâ€œåœ°çƒä¹‹å·…â€ï¼Œæ˜¯åœ°çƒä¸Šæœ€é«˜çš„å±±å³°ã€‚ç ç©†æœ—ç›å³°æ˜¯ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°ï¼Œæµ·æ‹”8,848ç±³ï¼Œä½äºå–œé©¬æ‹‰é›…å±±è„‰ä¸­ï¼Œæ˜¯åœ°çƒä¸Šæœ€é«˜çš„å±±å³°ã€‚\n",
      "\n",
      "\n",
      "\n",
      "ğŸ‘¶: äºŒæ°§åŒ–ç¢³åœ¨ç©ºæ°”ä¸­\n",
      "ğŸ¤–ï¸: ä»¥å‡å°‘å…¶äºŒæ°§åŒ–ç¢³å«é‡çš„æ–¹å¼ç”Ÿæˆæ°§æ°”ã€‚äºŒæ°§åŒ–ç¢³çš„åŒ–å­¦å¼ä¸ºH2Oï¼Œå…¶ä¸­CO2å’ŒCO2åœ¨ç©ºæ°”ä¸­ä»¥å‡å°‘å…¶äºŒæ°§åŒ–ç¢³å«é‡çš„æ–¹å¼ç”Ÿæˆæ°§æ°”ã€‚\n",
      "åœ¨ç©ºæ°”ä¸­ï¼ŒäºŒæ°§åŒ–ç¢³çš„å«é‡å‡å°‘äº†ï¼Œå¯¼è‡´äº†äºŒæ°§åŒ–ç¢³çš„å«é‡å¢åŠ ã€‚CO2å’ŒCO2åœ¨ç©ºæ°”ä¸­ä»¥å‡å°‘å…¶äºŒæ°§åŒ–ç¢³å«é‡çš„æ–¹å¼ç”Ÿæˆæ°§æ°”ã€‚\n",
      "\n",
      "\n",
      "\n",
      "ğŸ‘¶: åœ°çƒä¸Šæœ€å¤§çš„åŠ¨ç‰©æœ‰\n",
      "ğŸ¤–ï¸: å¤šå°‘ä¸ªï¼Ÿåœ°çƒä¸Šæœ€å¤§çš„åŠ¨ç‰©æ˜¯é²¸é±¼ï¼Œå®ƒä»¬æœ‰52ä¸ªå¤´ï¼Œæœ€é•¿çš„å¯è¾¾72å…¬é‡Œã€‚é²¸é±¼æ˜¯åœ°çƒä¸Šæœ€å¤§çš„å“ºä¹³åŠ¨ç‰©ï¼Œå®ƒä»¬æ˜¯æœ€å¤§çš„åŠ¨ç‰©ï¼Œå¯ä»¥åœ¨æ°´ä¸­æ¸¸æ³³å’Œç¹æ®–ã€‚é²¸é±¼æ˜¯ä¸–ç•Œä¸Šæœ€å¤§çš„åŠ¨ç‰©ï¼Œå¯ä»¥é•¿è¾¾30ç±³ï¼Œé‡é‡å¯è¾¾500å…¬æ–¤ã€‚\n",
      "\n",
      "\n",
      "\n",
      "ğŸ‘¶: æ­å·å¸‚çš„ç¾é£Ÿæœ‰\n",
      "ğŸ¤–ï¸: å“ªäº›ï¼Ÿæ­å·å¸‚çš„ç¾é£Ÿæœ‰å¾ˆå¤šï¼Œå…¶ä¸­ä¸€äº›æœ€æœ‰åçš„åŒ…æ‹¬ï¼š\n",
      "1. æ­å·çƒ¤é¸­ï¼šè¿™ä¸ªèœè‚´æ˜¯æ­å·æœ€è‘—åçš„é¸­è‚‰ï¼Œä»¥å…¶ç‹¬ç‰¹çš„å£æ„Ÿå’Œé…¥è„†çš„çš®è–„é¦…å’Œé²œç¾çš„è‚‰æ±è€Œè‘—åã€‚\n",
      "2. æ­å·éº»å©†è±†è…ï¼šè¿™ä¸ªè±†è…æ˜¯å½“åœ°çš„ä¸€é“ä¼ ç»Ÿå·èœï¼Œä»¥è±†è…å’Œè±†è…ä¸ºä¸»è¦åŸæ–™ï¼Œç»è¿‡çƒ¹åˆ¶åå˜å¾—æ›´åŠ çš„è¾£é¦™å‘³ã€‚\n",
      "3. æ­å·ç‚’é¥­ï¼šè¿™ä¸ªèœè‚´ä¸»è¦æ˜¯ä»¥ç”Ÿèœã€é¸¡è‚‰ã€è±†è…å’Œè›‹ç­‰ä¸ºä¸»ï¼Œé…ä»¥å„ç§è°ƒå‘³æ–™å’Œé…±æ–™ï¼Œå£æ„Ÿä¸°å¯Œã€‚\n",
      "4. æ­å·ç‚–èœï¼šè¿™ä¸ªèœè‚´ä»¥è‚‰è´¨é²œå«©ã€å£æ„Ÿæä½³çš„ç‰¹ç‚¹ï¼Œæ˜¯ä¸€é“ä»¥çŒªè‚‰å’Œè±†è…ä¸ºä¸»è¦åŸæ–™çš„ç¾é£Ÿã€‚\n",
      "5. æ¹–å—èœï¼šè¿™ä¸ªèœè‚´ä»¥æ¹–å—èœä¸ºä¸»ï¼Œä»¥å…¶ç‹¬ç‰¹çš„æ±¤åº•å’Œå„ç§è‚‰ç±»ä¸ºä¸»è¦åŸæ–™ï¼Œå£æ„Ÿé²œç¾ã€‚\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = init_model(EvalArgs)\n",
    "\n",
    "prompts = get_prompt_datas(EvalArgs)\n",
    "test_mode = int(input('[0] è‡ªåŠ¨æµ‹è¯•\\n[1] æ‰‹åŠ¨è¾“å…¥\\n'))\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "messages = []\n",
    "for idx, prompt in enumerate(prompts if test_mode == 0 else iter(lambda: input('ğŸ‘¶: '), '')):\n",
    "    \n",
    "    if test_mode == 0: print(f'ğŸ‘¶: {prompt}')\n",
    "\n",
    "    messages = messages[-EvalArgs.history_cnt:] if EvalArgs.history_cnt else []\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    new_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    ) if EvalArgs.model_mode != 0 else (tokenizer.bos_token + prompt)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        new_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    ).to(EvalArgs.device)\n",
    "\n",
    "    print('ğŸ¤–ï¸: ', end='')\n",
    "    generated_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=EvalArgs.max_seq_len,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        streamer=streamer,\n",
    "        top_p=EvalArgs.top_p,\n",
    "        temperature=EvalArgs.temperature\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(generated_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    print('\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
