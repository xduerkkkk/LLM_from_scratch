{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30403/2837662099.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx = nullcontext() if device == \"cpu\" else torch.cuda.amp.autocast()\n"
     ]
    }
   ],
   "source": [
    "class TrainArgs():\n",
    "    #文件管理\n",
    "    out_dir = './pretrain_output'\n",
    "    checkpoint_path = \"./pretrain_output/latest_checkpoint.pth\"\n",
    "    data_path = 'data/pretrain_hq.jsonl'\n",
    "    #神经网络训练管理\n",
    "    epochs = 2\n",
    "    batch_size = 32\n",
    "    accumulation_steps = 4\n",
    "    learning_rate = 5e-4\n",
    "    warm_up = 0\n",
    "    grad_clip = 1\n",
    "    dtype = 'bfloat16'\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers = min(8, os.cpu_count()) \n",
    "    log_interval = 50\n",
    "    save_interval = 2000\n",
    "    ctx = nullcontext() if device == \"cpu\" else torch.cuda.amp.autocast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalArgs():\n",
    "    hidden_size = 512\n",
    "    temperature = 0.85\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    num_hidden_layers = 8\n",
    "    history_cnt = 0\n",
    "    model_mode = 0 #0: 预训练模型，1: SFT-Chat模型，2: RLHF-Chat模型，3: Reason模型，4: RLAIF-Chat模型\"\n",
    "    max_seq_len = 8192\n",
    "    top_p =0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(args):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./model/')\n",
    "    \n",
    "        \n",
    "    modes = {0: 'pretrain', 1: 'full_sft', 2: 'rlhf', 3: 'reason', 4: 'grpo'}\n",
    "    ckp = f'{modes[args.model_mode]}_output/latest_checkpoint.pth'\n",
    "\n",
    "    model = MiniMindForCausalLM(MiniMindConfig(\n",
    "        hidden_size=args.hidden_size,\n",
    "        num_hidden_layers=args.num_hidden_layers,\n",
    "        \n",
    "    ))\n",
    "\n",
    "    # 1. 加载完整的 checkpoint 字典，并指明 weights_only=False\n",
    "    print(f\"Loading checkpoint from {ckp}...\")\n",
    "    checkpoint = torch.load(ckp, map_location=args.device, weights_only=False)\n",
    "    \n",
    "    # 2. 从字典中提取模型的 state_dict 来加载\n",
    "    model.load_state_dict(checkpoint['model'], strict=True)\n",
    "    \n",
    "    print(\"Checkpoint loaded successfully.\")\n",
    "    return model.eval().to(args.device), tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_datas(args):\n",
    "    if args.model_mode == 0:\n",
    "        # pretrain模型的接龙能力（无法对话）\n",
    "        prompt_datas = [\n",
    "            '马克思主义基本原理',\n",
    "            '人类大脑的主要功能',\n",
    "            '万有引力原理是',\n",
    "            '世界上最高的山峰是',\n",
    "            '二氧化碳在空气中',\n",
    "            '地球上最大的动物有',\n",
    "            '杭州市的美食有'\n",
    "        ]\n",
    "    else:\n",
    "       \n",
    "        # 通用对话问题\n",
    "        prompt_datas = [\n",
    "            '请介绍一下自己。',\n",
    "            '你更擅长哪一个学科？',\n",
    "            '鲁迅的《狂人日记》是如何批判封建礼教的？',\n",
    "            '我咳嗽已经持续了两周，需要去医院检查吗？',\n",
    "            '详细的介绍光速的物理概念。',\n",
    "            '推荐一些杭州的特色美食吧。',\n",
    "            '请为我讲解“大语言模型”这个概念。',\n",
    "            '如何理解ChatGPT？',\n",
    "            'Introduce the history of the United States, please.'\n",
    "        ]\n",
    "        \n",
    "            \n",
    "\n",
    "    return prompt_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正式评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from pretrain_output/latest_checkpoint.pth...\n",
      "Checkpoint loaded successfully.\n",
      "👶: 马克思主义基本原理\n",
      "🤖️: 是什么？马克思主义基本原理是将人类智慧和创造力集中在人类的思维能力之外，即通过科学方法和思维手段，在不同的历史时期或文化背景下，将人类智慧与创造力相结合，从而实现人类文明的进步和发展。\n",
      "\n",
      "\n",
      "\n",
      "👶: 人类大脑的主要功能\n",
      "🤖️: 是什么？ 人类大脑主要功能是通过神经元之间的连接来进行信息处理和信息传递。在大脑中，神经元通过神经元之间的连接产生信息，这些信息传递给大脑。神经元通过这些神经元连接成神经元，这些神经元之间通过电信号传递信息。神经元是神经系统的重要组成部分，它们通过神经元之间的信号传递，并传递到大脑的各个部位。神经元之间的连接可以是感知、运动、学习、记忆等多种功能，神经元之间的相互作用可以影响认知和行为。\n",
      "\n",
      "\n",
      "\n",
      "👶: 万有引力原理是\n",
      "🤖️: 如何运作的？万有引力原理是通过引力作用力来描述的。\n",
      "\n",
      "引力是由质量和距离之间产生的，这些距离与距离的平方成反比。\n",
      "\n",
      "当两个物体之间存在引力时，它们之间的引力会发生变化，这个变化的结果就是它们之间相互吸引的关系。引力会将物体吸引到它们的轨道，这会影响它们之间的引力。\n",
      "\n",
      "当两个物体之间存在引力时，它们的引力会发生变化，例如，当两个物体受到一个力时，它们之间的引力会改变它们之间的距离。如果两个物体之间的引力不同，它们之间的引力也会产生变化，例如，当两个物体的质量增加时，它们之间的引力会减少。\n",
      "\n",
      "\n",
      "\n",
      "👶: 世界上最高的山峰是\n",
      "🤖️: 珠穆朗玛峰，海拔8,848米。它位于喜马拉雅山脉中，被称为“地球之巅”，是地球上最高的山峰。珠穆朗玛峰是世界上最高的山峰，海拔8,848米，位于喜马拉雅山脉中，是地球上最高的山峰。\n",
      "\n",
      "\n",
      "\n",
      "👶: 二氧化碳在空气中\n",
      "🤖️: 以减少其二氧化碳含量的方式生成氧气。二氧化碳的化学式为H2O，其中CO2和CO2在空气中以减少其二氧化碳含量的方式生成氧气。\n",
      "在空气中，二氧化碳的含量减少了，导致了二氧化碳的含量增加。CO2和CO2在空气中以减少其二氧化碳含量的方式生成氧气。\n",
      "\n",
      "\n",
      "\n",
      "👶: 地球上最大的动物有\n",
      "🤖️: 多少个？地球上最大的动物是鲸鱼，它们有52个头，最长的可达72公里。鲸鱼是地球上最大的哺乳动物，它们是最大的动物，可以在水中游泳和繁殖。鲸鱼是世界上最大的动物，可以长达30米，重量可达500公斤。\n",
      "\n",
      "\n",
      "\n",
      "👶: 杭州市的美食有\n",
      "🤖️: 哪些？杭州市的美食有很多，其中一些最有名的包括：\n",
      "1. 杭州烤鸭：这个菜肴是杭州最著名的鸭肉，以其独特的口感和酥脆的皮薄馅和鲜美的肉汁而著名。\n",
      "2. 杭州麻婆豆腐：这个豆腐是当地的一道传统川菜，以豆腐和豆腐为主要原料，经过烹制后变得更加的辣香味。\n",
      "3. 杭州炒饭：这个菜肴主要是以生菜、鸡肉、豆腐和蛋等为主，配以各种调味料和酱料，口感丰富。\n",
      "4. 杭州炖菜：这个菜肴以肉质鲜嫩、口感极佳的特点，是一道以猪肉和豆腐为主要原料的美食。\n",
      "5. 湖南菜：这个菜肴以湖南菜为主，以其独特的汤底和各种肉类为主要原料，口感鲜美。\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = init_model(EvalArgs)\n",
    "\n",
    "prompts = get_prompt_datas(EvalArgs)\n",
    "test_mode = int(input('[0] 自动测试\\n[1] 手动输入\\n'))\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "messages = []\n",
    "for idx, prompt in enumerate(prompts if test_mode == 0 else iter(lambda: input('👶: '), '')):\n",
    "    \n",
    "    if test_mode == 0: print(f'👶: {prompt}')\n",
    "\n",
    "    messages = messages[-EvalArgs.history_cnt:] if EvalArgs.history_cnt else []\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    new_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    ) if EvalArgs.model_mode != 0 else (tokenizer.bos_token + prompt)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        new_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    ).to(EvalArgs.device)\n",
    "\n",
    "    print('🤖️: ', end='')\n",
    "    generated_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=EvalArgs.max_seq_len,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        streamer=streamer,\n",
    "        top_p=EvalArgs.top_p,\n",
    "        temperature=EvalArgs.temperature\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(generated_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    print('\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
