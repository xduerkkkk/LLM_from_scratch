{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数的配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkkkkk\\AppData\\Local\\Temp\\ipykernel_98792\\3167644256.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx = nullcontext() if device == \"cpu\" else torch.cuda.amp.autocast()\n"
     ]
    }
   ],
   "source": [
    "from model.model_minimind import MiniMindConfig\n",
    "class TrainArgs():\n",
    "    #文件管理\n",
    "    out_dir = './DPO_output'\n",
    "    checkpoint_path = \"./DPO_output/latest_checkpoint.pth\"\n",
    "    data_path = '../data/dpo.jsonl'\n",
    "    #神经网络训练管理\n",
    "    epochs = 2\n",
    "    batch_size = 16\n",
    "    accumulation_steps = 4\n",
    "    learning_rate = 5e-4\n",
    "    warm_up = 0\n",
    "    grad_clip = 1\n",
    "    dtype = 'bfloat16'\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers = 0\n",
    "    log_interval = 50\n",
    "    save_interval = 2000\n",
    "    ctx = nullcontext() if device == \"cpu\" else torch.cuda.amp.autocast()\n",
    "class LLMargs():\n",
    "    use_moe = True\n",
    "    hidden_size = 512\n",
    "    num_hidden_layers = 8\n",
    "    \n",
    "\n",
    "lm_config = MiniMindConfig(use_moe=LLMargs.use_moe,hidden_size=LLMargs.hidden_size,num_hidden_layers=LLMargs.num_hidden_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境与设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "Number of GPUs: 1\n",
      "  GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "    Memory: 8.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型加载、数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained('../model/')\n",
    "model = MiniMindForCausalLM(lm_config).to(TrainArgs.device)\n",
    "ref_model = MiniMindForCausalLM(lm_config).to(TrainArgs.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp = './DPO_output/latest_checkpoint.pth'\n",
    "state_dict = torch.load(ckp, map_location=TrainArgs.device)\n",
    "\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "ref_model.load_state_dict(state_dict, strict=False) # 加载同样的SFT权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model.eval() # 切换到评估模式\n",
    "ref_model.requires_grad_(False) # 关闭所有参数的梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# 打印结果，使用 f-string 的格式化功能让数字更易读（例如，加上千位分隔符）\n",
    "print(f\"模型总参数量 (Total Parameters): {total_params:,}\")\n",
    "\n",
    "# 如果你还想区分“可训练参数”，可以这样做：\n",
    "# 在大多数情况下，总参数量和可训练参数量是一样的。\n",
    "# 除非你手动设置了某些参数的 requires_grad=False (冻结了某些层)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"可训练参数量 (Trainable Parameters): {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import json\n",
    "class DPODataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=4096):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.padding = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "        self.bos_id = tokenizer('<|im_start|>assistant', add_special_tokens=False).input_ids\n",
    "        self.eos_id = tokenizer('<|im_end|>', add_special_tokens=False).input_ids\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                obj = json.loads(line)\n",
    "                self.data.append(obj)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        chosen = item['chosen']  # 是一个 list，里面包含若干 {role, content}\n",
    "        rejected = item['rejected']  # 同上\n",
    "        chosen_prompt = self.tokenizer.apply_chat_template(\n",
    "            chosen, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "\n",
    "        rejected_prompt = self.tokenizer.apply_chat_template(\n",
    "            rejected, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        chosen_encoding = self.tokenizer(\n",
    "            chosen_prompt, truncation=True, max_length=self.max_length, padding='max_length'\n",
    "        )\n",
    "        rejected_encoding = self.tokenizer(\n",
    "            rejected_prompt, truncation=True, max_length=self.max_length, padding='max_length'\n",
    "        )\n",
    "\n",
    "        chosen_input_ids = chosen_encoding['input_ids']\n",
    "        chosen_loss_mask = self._generate_loss_mask(chosen_input_ids)\n",
    "\n",
    "        rejected_input_ids = rejected_encoding['input_ids']\n",
    "        rejected_loss_mask = self._generate_loss_mask(rejected_input_ids)\n",
    "        x_chosen = torch.tensor(chosen_input_ids[:-1], dtype=torch.long)\n",
    "        y_chosen = torch.tensor(chosen_input_ids[1:], dtype=torch.long)\n",
    "        mask_chosen = torch.tensor(chosen_loss_mask[1:], dtype=torch.long)\n",
    "        x_rejected = torch.tensor(rejected_input_ids[:-1], dtype=torch.long)\n",
    "        y_rejected = torch.tensor(rejected_input_ids[1:], dtype=torch.long)\n",
    "        mask_rejected = torch.tensor(rejected_loss_mask[1:], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'x_chosen': x_chosen,\n",
    "            'y_chosen': y_chosen,\n",
    "            'mask_chosen': mask_chosen,\n",
    "            'x_rejected': x_rejected,\n",
    "            'y_rejected': y_rejected,\n",
    "            'mask_rejected': mask_rejected\n",
    "        }\n",
    "\n",
    "    def _generate_loss_mask(self, input_ids):\n",
    "        loss_mask = [0] * len(input_ids)\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            if input_ids[i:i + len(self.bos_id)] == self.bos_id:\n",
    "                start = i + len(self.bos_id)\n",
    "                end = start\n",
    "                while end < len(input_ids):\n",
    "                    if input_ids[end:end + len(self.eos_id)] == self.eos_id:\n",
    "                        break\n",
    "                    end += 1\n",
    "                for j in range(start + 1, min(end + len(self.eos_id) + 1, self.max_length)):\n",
    "                    loss_mask[j] = 1\n",
    "                i = end + len(self.eos_id) if end < len(input_ids) else len(input_ids)\n",
    "            else:\n",
    "                i += 1\n",
    "        return loss_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds =DPODataset(TrainArgs.data_path,tokenizer,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_chosen': tensor([   1,   85,  736,  201,   59,  292,  389,  260, 3836, 1861,  501,    2,\n",
      "         201,    1,  320,  275,  201, 1701,  260, 4324,   82,  356, 6100,  566,\n",
      "        6395,   14,  276, 3792, 1185,  303,  260,  823,  275,  370,  365, 1519,\n",
      "         848,  290,  276,  271,  497,  569,  779,  356,  303,  276, 3717,  303,\n",
      "        5301,  823,  298, 1215, 4912,   77,   16, 2675, 2472,   80,   67, 3034,\n",
      "        2903, 5301,  260, 4912,   77,  281,  311,  754,   85, 2488, 4406,   18,\n",
      "          14,  822, 1353, 5301, 1214,  689,   78,  291,   67,  823,  290,  311,\n",
      "         754, 2488,   22,  704, 1215, 4912,   77,   33,    2,  201,    1, 1078,\n",
      "         538,  501,  201,   41, 2382,  349,  276, 3792, 1185,  303,  260,  823,\n",
      "         275,  370,  365, 1519,  848,  290,  276,  271,  497,  569,  779,  356,\n",
      "         303,  276, 3717,  303, 5301,  823,  298, 1215, 4912,   77,   14,  670,\n",
      "         343, 3295,  935, 4429,  432,   28,  201,  201,   62,   61, 5217, 5184,\n",
      "          93,   53,  285, 1185,   95,  901,  941, 5217,   86, 2557, 5217,   85,\n",
      "          83,   84,   86,   93,   62, 5184,   93,   42, 4285,  688, 4126,  298,\n",
      "          95,   95, 5217,   63,  201,  201,   89, 3241, 5217,   10,   77,   62,\n",
      "          11,  370,  276, 3538,  501,  303,  365, 1519,  848,  465,   16,  201,\n",
      "         201,   40,  664,  276, 1271, 4755,   14, 2472,   80,   67, 3034, 2903,\n",
      "        5301,  260, 4912,   77,  281,  311,  754,   85, 2488, 4406,   18,   16,\n",
      "        2422,  343,  813,  935, 1271,  290, 1716,  276, 4688,  303, 5217,   10,\n",
      "          77,   62, 2566,  201,  201,   62,   61,  730, 4907,  901,  941, 5217,\n",
      "          86, 2557, 5217,   85,   83,   84,   86,   93,   26,   95, 5217,   63,\n",
      "         201,  201,   62,   61,  730, 4907,  901,  941, 5217,   86, 2557,  864,\n",
      "          62,   85,   83,   84,   86,   93,   20,   95, 5217,   63,  201,  201,\n",
      "          62,   61,  941,  901, 5217, 6312,   93, 4406,   18, 6228,   20,   62,\n",
      "          85,   83,   84,   86,   93,   20,   95,   95,  901, 5217, 6312,   93,\n",
      "        4406,   18, 6228,   20, 5217,   86, 2557,  730,   16,   22, 4806,   95,\n",
      "         901, 5217, 6312,   93, 4406,   18, 6228,   20,   16,   26,   20,   26,\n",
      "          95, 5217, 3414,  306,   90, 1611,   24,   16,   23,   25, 5217,   63,\n",
      "         201,  201,   53,   81,   14,  276, 3538,  501,  303,  365, 1519,  848,\n",
      "         465, 5217,   10,   77,   62,   11,  370, 1847,   90, 4402, 1611,   24,\n",
      "          16,   23,   25,   16,  201,  201,   48,  409,   14,  670, 1051,  290,\n",
      "        1716,  939,  822, 1353, 5301,  689,   78,  291,   67, 1214,  823,  290,\n",
      "         311,  754, 2488,   22,  704, 1215, 4912,   77,   16, 2422,  343,  813,\n",
      "         276, 4031, 2306,  395,   67,   28,  201,  201,   62,   61, 1714,  704,\n",
      "         901, 1611,   24,   16,   23,   25, 5217,   86, 2557, 5217,   85,   83,\n",
      "          84,   86,   93,   62, 5184,   93,   42, 4285,  688, 4126,  298,   95,\n",
      "          95, 5217,   63,  201,  201,   62,   61, 5217,   85,   83,   84,   86,\n",
      "          93,   62, 5184,   93,   42, 4285,  688, 4126,  298,   95,   95,  901,\n",
      "        5217, 6312,   93,   22,  704, 6228,   23,   24,   16,   23,   25,   95,\n",
      "        5217, 3414,  306,   90, 2949,   16,   18,   25, 5217,   63,  201,  201,\n",
      "          62,   61, 5217, 5184,   93,   42, 4285,  688, 4126,  298,   95,  901,\n",
      "         958,   25,   16,   18,   25,   11,   64,   20, 5217, 3414,  306,   90,\n",
      "        1714,   27,   16,   27,   26, 5217,   63]), 'y_chosen': tensor([  85,  736,  201,   59,  292,  389,  260, 3836, 1861,  501,    2,  201,\n",
      "           1,  320,  275,  201, 1701,  260, 4324,   82,  356, 6100,  566, 6395,\n",
      "          14,  276, 3792, 1185,  303,  260,  823,  275,  370,  365, 1519,  848,\n",
      "         290,  276,  271,  497,  569,  779,  356,  303,  276, 3717,  303, 5301,\n",
      "         823,  298, 1215, 4912,   77,   16, 2675, 2472,   80,   67, 3034, 2903,\n",
      "        5301,  260, 4912,   77,  281,  311,  754,   85, 2488, 4406,   18,   14,\n",
      "         822, 1353, 5301, 1214,  689,   78,  291,   67,  823,  290,  311,  754,\n",
      "        2488,   22,  704, 1215, 4912,   77,   33,    2,  201,    1, 1078,  538,\n",
      "         501,  201,   41, 2382,  349,  276, 3792, 1185,  303,  260,  823,  275,\n",
      "         370,  365, 1519,  848,  290,  276,  271,  497,  569,  779,  356,  303,\n",
      "         276, 3717,  303, 5301,  823,  298, 1215, 4912,   77,   14,  670,  343,\n",
      "        3295,  935, 4429,  432,   28,  201,  201,   62,   61, 5217, 5184,   93,\n",
      "          53,  285, 1185,   95,  901,  941, 5217,   86, 2557, 5217,   85,   83,\n",
      "          84,   86,   93,   62, 5184,   93,   42, 4285,  688, 4126,  298,   95,\n",
      "          95, 5217,   63,  201,  201,   89, 3241, 5217,   10,   77,   62,   11,\n",
      "         370,  276, 3538,  501,  303,  365, 1519,  848,  465,   16,  201,  201,\n",
      "          40,  664,  276, 1271, 4755,   14, 2472,   80,   67, 3034, 2903, 5301,\n",
      "         260, 4912,   77,  281,  311,  754,   85, 2488, 4406,   18,   16, 2422,\n",
      "         343,  813,  935, 1271,  290, 1716,  276, 4688,  303, 5217,   10,   77,\n",
      "          62, 2566,  201,  201,   62,   61,  730, 4907,  901,  941, 5217,   86,\n",
      "        2557, 5217,   85,   83,   84,   86,   93,   26,   95, 5217,   63,  201,\n",
      "         201,   62,   61,  730, 4907,  901,  941, 5217,   86, 2557,  864,   62,\n",
      "          85,   83,   84,   86,   93,   20,   95, 5217,   63,  201,  201,   62,\n",
      "          61,  941,  901, 5217, 6312,   93, 4406,   18, 6228,   20,   62,   85,\n",
      "          83,   84,   86,   93,   20,   95,   95,  901, 5217, 6312,   93, 4406,\n",
      "          18, 6228,   20, 5217,   86, 2557,  730,   16,   22, 4806,   95,  901,\n",
      "        5217, 6312,   93, 4406,   18, 6228,   20,   16,   26,   20,   26,   95,\n",
      "        5217, 3414,  306,   90, 1611,   24,   16,   23,   25, 5217,   63,  201,\n",
      "         201,   53,   81,   14,  276, 3538,  501,  303,  365, 1519,  848,  465,\n",
      "        5217,   10,   77,   62,   11,  370, 1847,   90, 4402, 1611,   24,   16,\n",
      "          23,   25,   16,  201,  201,   48,  409,   14,  670, 1051,  290, 1716,\n",
      "         939,  822, 1353, 5301,  689,   78,  291,   67, 1214,  823,  290,  311,\n",
      "         754, 2488,   22,  704, 1215, 4912,   77,   16, 2422,  343,  813,  276,\n",
      "        4031, 2306,  395,   67,   28,  201,  201,   62,   61, 1714,  704,  901,\n",
      "        1611,   24,   16,   23,   25, 5217,   86, 2557, 5217,   85,   83,   84,\n",
      "          86,   93,   62, 5184,   93,   42, 4285,  688, 4126,  298,   95,   95,\n",
      "        5217,   63,  201,  201,   62,   61, 5217,   85,   83,   84,   86,   93,\n",
      "          62, 5184,   93,   42, 4285,  688, 4126,  298,   95,   95,  901, 5217,\n",
      "        6312,   93,   22,  704, 6228,   23,   24,   16,   23,   25,   95, 5217,\n",
      "        3414,  306,   90, 2949,   16,   18,   25, 5217,   63,  201,  201,   62,\n",
      "          61, 5217, 5184,   93,   42, 4285,  688, 4126,  298,   95,  901,  958,\n",
      "          25,   16,   18,   25,   11,   64,   20, 5217, 3414,  306,   90, 1714,\n",
      "          27,   16,   27,   26, 5217,   63,  201]), 'mask_chosen': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1]), 'x_rejected': tensor([   1,   85,  736,  201,   59,  292,  389,  260, 3836, 1861,  501,    2,\n",
      "         201,    1,  320,  275,  201, 1701,  260, 4324,   82,  356, 6100,  566,\n",
      "        6395,   14,  276, 3792, 1185,  303,  260,  823,  275,  370,  365, 1519,\n",
      "         848,  290,  276,  271,  497,  569,  779,  356,  303,  276, 3717,  303,\n",
      "        5301,  823,  298, 1215, 4912,   77,   16, 2675, 2472,   80,   67, 3034,\n",
      "        2903, 5301,  260, 4912,   77,  281,  311,  754,   85, 2488, 4406,   18,\n",
      "          14,  822, 1353, 5301, 1214,  689,   78,  291,   67,  823,  290,  311,\n",
      "         754, 2488,   22,  704, 1215, 4912,   77,   33,    2,  201,    1, 1078,\n",
      "         538,  501,  201, 5042, 6318,  935, 5242,   14,  670, 1051,  290, 2987,\n",
      "        1447, 1750,  764, 3220, 1519,  848,    4, 3879, 3841,  262, 6045, 2549,\n",
      "          16, 2340, 1519,  848,  465, 3879,  349, 1508,  980,  501,  465,  503,\n",
      "        5127, 4540,  369,  427, 4019,  980,  501,  465,   16, 1494, 1286,   14,\n",
      "        1309, 3143,  980,  501,  793, 3655,  281,  353,  389, 6351,  290,  340,\n",
      "         365, 1519,  848,   14, 3322, 1072,  417, 2353,  260, 3538,  501,  941,\n",
      "         751,  349,  353,   31,   77,   90,   16, 2251,  201,   48,  409,   14,\n",
      "        4755,  349,  276, 3792, 1185,  303,  260,  823,  275,  370,  365, 1519,\n",
      "         848,  290,  276,  271,  497,  569,  779,  356,  303,  276, 3717,  303,\n",
      "        5301,  823,  298, 1215, 4912,   77,   14,  670,  343, 4682,  935, 4429,\n",
      "         432, 2267,   85,   28,  201,  201,   53,  285, 1185,   31,   45,   12,\n",
      "          85,   83,   84,   86,   10,   48,   81,   16, 4883,  560, 4285,  688,\n",
      "        4126,  298,   11,  201,  201,   89, 3241, 1797,  370,  260, 3538,  501,\n",
      "        3658,  282,  276,  365, 1519,  848,  465, 2231,  279,   16, 2422,  804,\n",
      "        1338,  349, 2472,   80,   67, 3034, 2903, 5301,  260, 4912,   77,  281,\n",
      "         311,  754,   85, 2488, 4406,   18,   16, 2405, 1810,   14,  670,  343,\n",
      "        2560,  377, 4738,   71,  947, 3709, 1322, 1174, 2376,  337,  634, 3022,\n",
      "         290, 2092,   28,  201,  201,    6, 4406,   18,   31,   45,   12,   85,\n",
      "          83,   84,   86,   10,   26,   11,    6,  201,  201,   53,  393, 1521,\n",
      "         374, 1797,  362, 1518,  405,   28,  201,  201,   45,   31,    6,   62,\n",
      "        6312,   93, 4406,   18, 6228,   85,   83,   84,   86,   10,   26,   11,\n",
      "          95,    6,  201,  201,   48,  409,   14, 5770,  670, 2341,  290, 5913,\n",
      "         822, 1353, 5301,  689,   78,  291,   67, 2315,  290,  823,  290,  311,\n",
      "         754, 2488,   22,  704, 1215, 4912,   77,   14,  670,  343, 3199,  813,\n",
      "        1174, 2376,  337,  634, 3022, 1195, 4305, 2560,  377, 4738,  282, 2488,\n",
      "          22,  704, 6282,  303, 2488, 4406,   18,  281, 3336, 1521,  374,  933,\n",
      "          81,   16,   49,   72,   16,   42, 4285,   16,   57, 4126,  298,   28,\n",
      "         201,  201,    6,   22,  704,   31,   62, 6312,   93, 4406,   18, 6228,\n",
      "          85,   83,   84,   86,   10,   26,   11,   95,   12,   62,   85,   83,\n",
      "          84,   86,   93,   10,   48,   81,   16,   49,   72,   16,   42, 4285,\n",
      "          16,   57, 4126,  298,   11,   95,    6,  201,  201,   52, 1442,   84,\n",
      "         855,  282, 5317,  353, 2028, 1553,   28,  201,  201,    6,   62,   85,\n",
      "          83,   84,   86,   93,   10,   48,   81,   16,   49,   72,   16,   42,\n",
      "        4285,   16,   57, 4126,  298,   11,   95,   31,   62, 6312,   93,   22,\n",
      "         704, 6228,   62, 6312,   93, 4406,   18]), 'y_rejected': tensor([  85,  736,  201,   59,  292,  389,  260, 3836, 1861,  501,    2,  201,\n",
      "           1,  320,  275,  201, 1701,  260, 4324,   82,  356, 6100,  566, 6395,\n",
      "          14,  276, 3792, 1185,  303,  260,  823,  275,  370,  365, 1519,  848,\n",
      "         290,  276,  271,  497,  569,  779,  356,  303,  276, 3717,  303, 5301,\n",
      "         823,  298, 1215, 4912,   77,   16, 2675, 2472,   80,   67, 3034, 2903,\n",
      "        5301,  260, 4912,   77,  281,  311,  754,   85, 2488, 4406,   18,   14,\n",
      "         822, 1353, 5301, 1214,  689,   78,  291,   67,  823,  290,  311,  754,\n",
      "        2488,   22,  704, 1215, 4912,   77,   33,    2,  201,    1, 1078,  538,\n",
      "         501,  201, 5042, 6318,  935, 5242,   14,  670, 1051,  290, 2987, 1447,\n",
      "        1750,  764, 3220, 1519,  848,    4, 3879, 3841,  262, 6045, 2549,   16,\n",
      "        2340, 1519,  848,  465, 3879,  349, 1508,  980,  501,  465,  503, 5127,\n",
      "        4540,  369,  427, 4019,  980,  501,  465,   16, 1494, 1286,   14, 1309,\n",
      "        3143,  980,  501,  793, 3655,  281,  353,  389, 6351,  290,  340,  365,\n",
      "        1519,  848,   14, 3322, 1072,  417, 2353,  260, 3538,  501,  941,  751,\n",
      "         349,  353,   31,   77,   90,   16, 2251,  201,   48,  409,   14, 4755,\n",
      "         349,  276, 3792, 1185,  303,  260,  823,  275,  370,  365, 1519,  848,\n",
      "         290,  276,  271,  497,  569,  779,  356,  303,  276, 3717,  303, 5301,\n",
      "         823,  298, 1215, 4912,   77,   14,  670,  343, 4682,  935, 4429,  432,\n",
      "        2267,   85,   28,  201,  201,   53,  285, 1185,   31,   45,   12,   85,\n",
      "          83,   84,   86,   10,   48,   81,   16, 4883,  560, 4285,  688, 4126,\n",
      "         298,   11,  201,  201,   89, 3241, 1797,  370,  260, 3538,  501, 3658,\n",
      "         282,  276,  365, 1519,  848,  465, 2231,  279,   16, 2422,  804, 1338,\n",
      "         349, 2472,   80,   67, 3034, 2903, 5301,  260, 4912,   77,  281,  311,\n",
      "         754,   85, 2488, 4406,   18,   16, 2405, 1810,   14,  670,  343, 2560,\n",
      "         377, 4738,   71,  947, 3709, 1322, 1174, 2376,  337,  634, 3022,  290,\n",
      "        2092,   28,  201,  201,    6, 4406,   18,   31,   45,   12,   85,   83,\n",
      "          84,   86,   10,   26,   11,    6,  201,  201,   53,  393, 1521,  374,\n",
      "        1797,  362, 1518,  405,   28,  201,  201,   45,   31,    6,   62, 6312,\n",
      "          93, 4406,   18, 6228,   85,   83,   84,   86,   10,   26,   11,   95,\n",
      "           6,  201,  201,   48,  409,   14, 5770,  670, 2341,  290, 5913,  822,\n",
      "        1353, 5301,  689,   78,  291,   67, 2315,  290,  823,  290,  311,  754,\n",
      "        2488,   22,  704, 1215, 4912,   77,   14,  670,  343, 3199,  813, 1174,\n",
      "        2376,  337,  634, 3022, 1195, 4305, 2560,  377, 4738,  282, 2488,   22,\n",
      "         704, 6282,  303, 2488, 4406,   18,  281, 3336, 1521,  374,  933,   81,\n",
      "          16,   49,   72,   16,   42, 4285,   16,   57, 4126,  298,   28,  201,\n",
      "         201,    6,   22,  704,   31,   62, 6312,   93, 4406,   18, 6228,   85,\n",
      "          83,   84,   86,   10,   26,   11,   95,   12,   62,   85,   83,   84,\n",
      "          86,   93,   10,   48,   81,   16,   49,   72,   16,   42, 4285,   16,\n",
      "          57, 4126,  298,   11,   95,    6,  201,  201,   52, 1442,   84,  855,\n",
      "         282, 5317,  353, 2028, 1553,   28,  201,  201,    6,   62,   85,   83,\n",
      "          84,   86,   93,   10,   48,   81,   16,   49,   72,   16,   42, 4285,\n",
      "          16,   57, 4126,  298,   11,   95,   31,   62, 6312,   93,   22,  704,\n",
      "        6228,   62, 6312,   93, 4406,   18, 6228]), 'mask_rejected': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1])}\n",
      "数据集总长度为207339\n"
     ]
    }
   ],
   "source": [
    "print(f'{train_ds[1]}\\n数据集总长度为{len(train_ds)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = Subset(train_ds,range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size = TrainArgs.batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = TrainArgs.num_workers,\n",
    "    pin_memory= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size = TrainArgs.batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = TrainArgs.num_workers,\n",
    "    pin_memory= True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import optim\n",
    "def get_lr(current_step, total_steps, lr):\n",
    "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))\n",
    "optimizer = optim.AdamW(model.parameters(), lr=TrainArgs.learning_rate)\n",
    "#据模型在训练数据上的表现来调整模型的参数  训练的一个大脑\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(TrainArgs.dtype in ['float16', 'bfloat16']))\n",
    "#自动混合精度训练工具 训练的一个助手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swanlab\n",
    "# 初始化swanlab，传入项目名、实验名等\n",
    "swanlab.init(\n",
    "    project=\"MiniMind-SFT\", \n",
    "    experiment_name=\"MiniMind-SFT\", \n",
    "    config=vars(TrainArgs()) # 将你的所有超参数配置一次性传给swanlab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是计算loss的特殊算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def logits_to_probs(logits, labels):\n",
    "    # logits shape: (batch_size, seq_len, vocab_size)\n",
    "    # labels shape: (batch_size, seq_len)\n",
    "    # probs shape: (batch_size, seq_len)\n",
    "    log_probs = F.log_softmax(logits, dim=2)\n",
    "    probs = torch.gather(log_probs, dim=2, index=labels.unsqueeze(2)).squeeze(-1)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def dpo_loss(ref_probs, probs, mask, beta):\n",
    "    # ref_probs 和 probs 都是 shape: (batch_size, seq_len)\n",
    "    # https://github.com/jingyaogong/minimind/issues/298\n",
    "    seq_lengths = mask.sum(dim=1, keepdim=True)  # (batch_size, 1)\n",
    "    ref_probs = (ref_probs * mask).sum(dim=1) / seq_lengths.squeeze()\n",
    "    probs = (probs * mask).sum(dim=1) / seq_lengths.squeeze()\n",
    "\n",
    "    # 将 chosen 和 rejected 数据分开\n",
    "    batch_size = ref_probs.shape[0]\n",
    "    chosen_ref_probs = ref_probs[:batch_size // 2]\n",
    "    reject_ref_probs = ref_probs[batch_size // 2:]\n",
    "    chosen_probs = probs[:batch_size // 2]\n",
    "    reject_probs = probs[batch_size // 2:]\n",
    "\n",
    "    pi_logratios = chosen_probs - reject_probs\n",
    "    ref_logratios = chosen_ref_probs - reject_ref_probs\n",
    "    logits = pi_logratios - ref_logratios\n",
    "    loss = -F.logsigmoid(beta * logits)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, model, train_loader, optimizer, scaler, lm_config):\n",
    "    #选择初始化损失函数！\n",
    "    model.train() # 确保模型处于训练模式\n",
    "\n",
    "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    start_time = time.time()#一会记录日志要用\n",
    "\n",
    "    iter_per_epoch = len(train_loader)#看一下一次数据有多长，计算学习率要用\n",
    "\n",
    "    # --- 新增：使用tqdm包装DataLoader以显示进度条 ---\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{TrainArgs.epochs}\", leave=True)\n",
    "\n",
    "    for step, batch in enumerate(train_loader):#不断提取数据\n",
    "        x_chosen = batch['x_chosen'].to(args.device)\n",
    "        x_rejected = batch['x_rejected'].to(args.device)\n",
    "        y_chosen = batch['y_chosen'].to(args.device)\n",
    "        y_rejected = batch['y_rejected'].to(args.device)\n",
    "        mask_chosen = batch['mask_chosen'].to(args.device)\n",
    "        mask_rejected = batch['mask_rejected'].to(args.device)\n",
    "        X = torch.cat([x_chosen, x_rejected], dim=0)\n",
    "        Y = torch.cat([y_chosen, y_rejected], dim=0)\n",
    "        loss_mask = torch.cat([mask_chosen, mask_rejected], dim=0)#数据上设备，上到gpu上\n",
    "\n",
    "        \n",
    "        lr = get_lr(epoch * iter_per_epoch + step, TrainArgs.epochs * iter_per_epoch, TrainArgs.learning_rate)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr                #计算当前学习率的算法\n",
    "\n",
    "        with TrainArgs.ctx:\n",
    "            with torch.no_grad():\n",
    "                ref_outputs = ref_model(X)\n",
    "                ref_logits = ref_outputs.logits\n",
    "            ref_probs = logits_to_probs(ref_logits, Y)\n",
    "            ref_probs = ref_probs * loss_mask\n",
    "            outputs = model(X)\n",
    "            logits = outputs.logits\n",
    "            probs = logits_to_probs(logits, Y)\n",
    "            probs = probs * loss_mask\n",
    "            loss = dpo_loss(ref_probs, probs, loss_mask, beta=0.1)\n",
    "            loss = loss / TrainArgs.accumulation_steps  \n",
    "\n",
    "            scaler.scale(loss).backward()  #反向传播\n",
    "\n",
    "        if (step + 1) % TrainArgs.accumulation_steps == 0:#判断梯度累加到位没有\n",
    "            scaler.unscale_(optimizer)#*前置操作\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), TrainArgs.grad_clip)\n",
    "            #梯度裁剪\n",
    "            scaler.step(optimizer)#参数更新\n",
    "            scaler.update()#*调整\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)#本次训练结束，清零梯度，为下一次反向传播准备\n",
    "\n",
    "        if step % TrainArgs.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            progress_bar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.3f}\",\n",
    "                \"lr\": f\"{current_lr:.2e}\"\n",
    "            })\n",
    "\n",
    "            if (swanlab is not None) : #用日志软件记录训练过程\n",
    "                swanlab.log({\"loss\": loss.item() * TrainArgs.accumulation_steps,\n",
    "                           \"lr\": optimizer.param_groups[-1]['lr'],\n",
    "                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n",
    "\n",
    "            # 保存checkpoint\n",
    "        if (step + 1) % TrainArgs.save_interval == 0:\n",
    "            # 只在更新步之后保存，确保梯度累积完成\n",
    "            if (step + 1) % TrainArgs.accumulation_steps == 0:\n",
    "                model.eval()\n",
    "                print(f\"\\nSaving checkpoint at step {step+1}...\")\n",
    "                \n",
    "                # 创建checkpoint字典\n",
    "                checkpoint = {\n",
    "                     # 对模型权重进行半精度转换\n",
    "                    'model': {k: v.half() for k, v in model.state_dict().items()},\n",
    "                    # 对优化器状态也进行转换（更严谨的做法）\n",
    "                    'optimizer': {\n",
    "                        'state': {k: v.half() if torch.is_tensor(v) else v for k, v in optimizer.state_dict()['state'].items()},\n",
    "                        'param_groups': optimizer.state_dict()['param_groups']\n",
    "                    },\n",
    "                    'scaler': scaler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'config': TrainArgs,\n",
    "                }\n",
    "                \n",
    "                # 保存\n",
    "                torch.save(checkpoint, TrainArgs.checkpoint_path)\n",
    "                print(\"Checkpoint saved.\")\n",
    "                model.train() # 切换回训练模式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现循环训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 关键：Checkpoint加载逻辑 ---\n",
    "start_epoch = 0\n",
    "start_step = 0\n",
    "if os.path.exists(TrainArgs.checkpoint_path):\n",
    "    print(f\"=> Resuming from checkpoint: {TrainArgs.checkpoint_path}\")\n",
    "    checkpoint = torch.load(TrainArgs.checkpoint_path, map_location=TrainArgs.device)\n",
    "    \n",
    "    # 加载模型权重 (处理DDP保存的权重)\n",
    "    model_state_dict = checkpoint['model']\n",
    "    if any(key.startswith('module.') for key in model_state_dict):\n",
    "            model_state_dict = {k.replace('module.', ''): v for k, v in model_state_dict.items()}\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    # 加载优化器和scaler状态\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scaler.load_state_dict(checkpoint['scaler'])\n",
    "    \n",
    "    # 恢复训练进度\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    # 注意：恢复step逻辑可以让dataloader跳过已训练数据，这里简化为从头开始当前epoch\n",
    "    print(f\"=> Resumed from epoch {start_epoch + 1}\")\n",
    "else:\n",
    "    print(\"=> Starting from scratch...\")\n",
    "\n",
    "model.to(TrainArgs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试训练运行以下代码\n",
    "for epoch in range(start_epoch, TrainArgs.epochs):\n",
    "        train_epoch(epoch, model, test_loader, optimizer, scaler, lm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正式训练运行以下代码\n",
    "for epoch in range(start_epoch, TrainArgs.epochs):\n",
    "        train_epoch(epoch, model, train_loader, optimizer, scaler, lm_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
